#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë¼ë²¨ ì—†ì´ ëª¨ë¸ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ë„êµ¬

ì‚¬ìš©ë²•:
    python tools/unsupervised_evaluator.py --video assets/simpson.mp4 --models MODEL1 MODEL2 MODEL3
"""

import argparse
import json
import numpy as np
from pathlib import Path
from collections import Counter, defaultdict
from scipy.stats import entropy
import matplotlib.pyplot as plt
import sys

# ìƒìœ„ ë””ë ‰í† ë¦¬ë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

from emotion_classifier import EmotionClassifier
import whisperx
import torch

class UnsupervisedEvaluator:
    """ë¼ë²¨ ì—†ì´ ëª¨ë¸ í’ˆì§ˆ í‰ê°€"""
    
    def __init__(self, models, device="auto"):
        self.models = models
        self.predictions = {}
        
        if device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
        
        print(f"ğŸ”§ Device: {self.device}")
    
    def load_segments(self, video_path):
        """ì˜ìƒì—ì„œ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ (WhisperX)"""
        print(f"\nğŸ“¹ Loading video: {video_path}")
        
        # WhisperXë¡œ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ
        compute_type = "float16" if self.device == "cuda" else "float32"
        model = whisperx.load_model("large-v2", self.device, compute_type=compute_type)
        
        audio = whisperx.load_audio(video_path)
        result = model.transcribe(audio, batch_size=16)
        
        segments = result['segments']
        print(f"âœ… Extracted {len(segments)} segments")
        
        return segments
    
    def predict_all_models(self, segments, video_path):
        """ëª¨ë“  ëª¨ë¸ë¡œ ì˜ˆì¸¡"""
        print(f"\nğŸ”„ Running predictions with {len(self.models)} models...")
        
        for model_name in self.models:
            print(f"\n  Model: {model_name}")
            try:
                classifier = EmotionClassifier(
                    audio_model_name=model_name,
                    device=self.device,
                    enable_text=False
                )
                
                # ê°ì • ë¶„ë¥˜ (ë°°ì¹˜ ì²˜ë¦¬)
                import librosa
                audio_path = video_path.replace('.mp4', '_temp_audio.wav')
                
                # ì„ì‹œ ì˜¤ë””ì˜¤ ì¶”ì¶œ (ì´ë¯¸ ìˆìœ¼ë©´ ì¬ì‚¬ìš©)
                if not Path(audio_path).exists():
                    audio = whisperx.load_audio(video_path)
                    import soundfile as sf
                    sf.write(audio_path, audio, 16000)
                
                # ë°°ì¹˜ ì²˜ë¦¬
                full_audio = librosa.load(audio_path, sr=16000)[0]
                results = classifier.process_batch(segments, full_audio)
                
                # ê²°ê³¼ ë³€í™˜
                predictions = []
                for i, (seg, result) in enumerate(zip(segments, results)):
                    predictions.append({
                        'segment_id': i,
                        'emotion': result.emotion,
                        'confidence': result.confidence,
                        'audio_score': result.audio_score,
                        'audio_distribution': result.audio_distribution,
                        'start': seg.get('start', 0),
                        'end': seg.get('end', 0),
                        'text': seg.get('text', '')
                    })
                
                self.predictions[model_name] = predictions
                print(f"    âœ… Completed: {len(predictions)} predictions")
                
            except Exception as e:
                print(f"    âŒ Model failed: {str(e)}")
                self.predictions[model_name] = []
    
    def calculate_consistency(self):
        """ëª¨ë¸ ê°„ ì˜ˆì¸¡ ì¼ì¹˜ë„ ê³„ì‚°"""
        if not self.predictions:
            return {}
        
        n_segments = len(list(self.predictions.values())[0])
        consistency_scores = {}
        
        for model_name in self.predictions:
            agreements = []
            
            for i in range(n_segments):
                # ì´ ì„¸ê·¸ë¨¼íŠ¸ì— ëŒ€í•œ ëª¨ë“  ëª¨ë¸ì˜ ì˜ˆì¸¡
                segment_predictions = []
                for m in self.predictions:
                    if i < len(self.predictions[m]):
                        segment_predictions.append(self.predictions[m][i]['emotion'])
                
                if not segment_predictions:
                    continue
                
                # ê°€ì¥ ë§ì€ ì˜ˆì¸¡ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                most_common = Counter(segment_predictions).most_common(1)[0][0]
                current_pred = self.predictions[model_name][i]['emotion']
                
                if current_pred == most_common:
                    agreements.append(1)
                else:
                    agreements.append(0)
            
            consistency_scores[model_name] = np.mean(agreements) if agreements else 0
        
        return consistency_scores
    
    def analyze_confidence(self, predictions):
        """ì‹ ë¢°ë„ ë¶„ì„"""
        if not predictions:
            return {}
        
        confidences = [p['confidence'] for p in predictions]
        
        return {
            'mean': np.mean(confidences),
            'median': np.median(confidences),
            'std': np.std(confidences),
            'high_conf_ratio': sum(1 for c in confidences if c > 0.7) / len(confidences),
            'low_conf_ratio': sum(1 for c in confidences if c < 0.4) / len(confidences)
        }
    
    def analyze_distribution(self, predictions):
        """ê°ì • ë¶„í¬ ë¶„ì„"""
        if not predictions:
            return {}
        
        emotion_counts = Counter([p['emotion'] for p in predictions])
        total = len(predictions)
        
        distribution = {
            emotion: count / total 
            for emotion, count in emotion_counts.items()
        }
        
        # ë‹¤ì–‘ì„± (Entropy)
        probs = list(distribution.values())
        diversity = entropy(probs, base=2)
        
        # Gini ê³„ìˆ˜
        sorted_probs = sorted(probs)
        n = len(sorted_probs)
        gini = sum((2 * i - n - 1) * p for i, p in enumerate(sorted_probs, 1)) / (n * sum(sorted_probs))
        
        return {
            'distribution': distribution,
            'neutral_ratio': distribution.get('neutral', 0),
            'diversity': diversity,
            'gini': gini,
            'dominant_emotion': max(distribution, key=distribution.get),
            'dominant_ratio': max(distribution.values())
        }
    
    def calculate_entropy_score(self, predictions):
        """ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ í’ˆì§ˆ ì ìˆ˜"""
        if not predictions:
            return 0
        
        scores = []
        for pred in predictions:
            dist = pred['audio_distribution']
            probs = list(dist.values())
            
            ent = entropy(probs, base=2)
            
            # ì´ìƒì  ì—”íŠ¸ë¡œí”¼: 1.0~2.0
            if 1.0 <= ent <= 2.0:
                quality = 1.0
            elif ent < 1.0:
                quality = ent / 1.0
            else:
                quality = 2.0 / ent
            
            scores.append(quality)
        
        return np.mean(scores)
    
    def evaluate(self):
        """ì¢…í•© í‰ê°€"""
        consistency = self.calculate_consistency()
        
        quality_scores = {}
        for model_name, preds in self.predictions.items():
            if not preds:
                continue
            
            confidence_metrics = self.analyze_confidence(preds)
            distribution_metrics = self.analyze_distribution(preds)
            entropy_score = self.calculate_entropy_score(preds)
            
            # ì¢…í•© ì ìˆ˜ ê³„ì‚°
            overall_score = (
                0.3 * consistency.get(model_name, 0.5) +
                0.3 * entropy_score +
                0.2 * (1 - distribution_metrics['neutral_ratio']) +
                0.2 * confidence_metrics['mean']
            )
            
            quality_scores[model_name] = {
                'overall_score': overall_score,
                'consistency': consistency.get(model_name, 0),
                'entropy_quality': entropy_score,
                'confidence': confidence_metrics,
                'distribution': distribution_metrics
            }
        
        # ìˆœìœ„ ë§¤ê¸°ê¸°
        ranked = sorted(quality_scores.items(), key=lambda x: x[1]['overall_score'], reverse=True)
        
        return ranked
    
    def print_results(self, ranked):
        """ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ“Š UNSUPERVISED EVALUATION RESULTS (ë¼ë²¨ ì—†ëŠ” í‰ê°€ ê²°ê³¼)")
        print("="*80)
        
        for rank, (model, scores) in enumerate(ranked, 1):
            print(f"\nğŸ† Rank {rank}: {model}")
            print(f"   Overall Score: {scores['overall_score']:.3f}")
            print(f"   â”œâ”€ Consistency (ëª¨ë¸ ê°„ ì¼ì¹˜ë„): {scores['consistency']:.3f}")
            print(f"   â”œâ”€ Entropy Quality (ì˜ˆì¸¡ í’ˆì§ˆ): {scores['entropy_quality']:.3f}")
            print(f"   â”œâ”€ Mean Confidence (í‰ê·  ì‹ ë¢°ë„): {scores['confidence']['mean']:.3f}")
            print(f"   â””â”€ Neutral Ratio (ì¤‘ë¦½ ë¹„ìœ¨): {scores['distribution']['neutral_ratio']:.3f}")
            
            print(f"\n   ğŸ“ˆ Emotion Distribution:")
            for emotion, ratio in sorted(scores['distribution']['distribution'].items(), 
                                         key=lambda x: x[1], reverse=True):
                bar = "â–ˆ" * int(ratio * 30)
                print(f"      {emotion:10s} {bar} {ratio:.3f}")
        
        print("\n" + "="*80)
    
    def save_results(self, ranked, output_path):
        """ê²°ê³¼ ì €ì¥"""
        results = {
            'ranked_models': [
                {
                    'rank': i,
                    'model': model,
                    'scores': scores
                }
                for i, (model, scores) in enumerate(ranked, 1)
            ]
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ Results saved: {output_path}")
    
    def visualize(self, ranked, output_path):
        """ì‹œê°í™”"""
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        
        # 1. Overall Score ë¹„êµ
        models = [m for m, _ in ranked]
        scores = [s['overall_score'] for _, s in ranked]
        
        axes[0, 0].barh(models, scores, color='steelblue')
        axes[0, 0].set_xlabel('Overall Score')
        axes[0, 0].set_title('Model Quality Ranking')
        axes[0, 0].set_xlim(0, 1)
        
        # 2. ì„¸ë¶€ ì§€í‘œ ë¹„êµ
        consistency_scores = [s['consistency'] for _, s in ranked]
        entropy_scores = [s['entropy_quality'] for _, s in ranked]
        
        x = np.arange(len(models))
        width = 0.35
        
        axes[0, 1].bar(x - width/2, consistency_scores, width, label='Consistency', color='orange')
        axes[0, 1].bar(x + width/2, entropy_scores, width, label='Entropy Quality', color='green')
        axes[0, 1].set_ylabel('Score')
        axes[0, 1].set_title('Detailed Metrics')
        axes[0, 1].set_xticks(x)
        axes[0, 1].set_xticklabels([m.split('/')[-1][:15] for m in models], rotation=45, ha='right')
        axes[0, 1].legend()
        
        # 3. ì‹ ë¢°ë„ ë¶„í¬
        mean_confs = [s['confidence']['mean'] for _, s in ranked]
        high_confs = [s['confidence']['high_conf_ratio'] for _, s in ranked]
        
        axes[1, 0].scatter(mean_confs, high_confs, s=100, alpha=0.6, c=range(len(models)), cmap='viridis')
        for i, model in enumerate(models):
            axes[1, 0].annotate(i+1, (mean_confs[i], high_confs[i]), 
                               fontsize=12, ha='center', va='center', color='white', weight='bold')
        axes[1, 0].set_xlabel('Mean Confidence')
        axes[1, 0].set_ylabel('High Confidence Ratio')
        axes[1, 0].set_title('Confidence Analysis')
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. ì¤‘ë¦½ ë¹„ìœ¨ vs ë‹¤ì–‘ì„±
        neutral_ratios = [s['distribution']['neutral_ratio'] for _, s in ranked]
        diversities = [s['distribution']['diversity'] for _, s in ranked]
        
        axes[1, 1].scatter(neutral_ratios, diversities, s=100, alpha=0.6, c=range(len(models)), cmap='plasma')
        for i, model in enumerate(models):
            axes[1, 1].annotate(i+1, (neutral_ratios[i], diversities[i]), 
                               fontsize=12, ha='center', va='center', color='white', weight='bold')
        axes[1, 1].set_xlabel('Neutral Ratio')
        axes[1, 1].set_ylabel('Diversity (Entropy)')
        axes[1, 1].set_title('Distribution Quality')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=150)
        print(f"ğŸ“Š Visualization saved: {output_path}")


def main():
    parser = argparse.ArgumentParser(description="ë¼ë²¨ ì—†ì´ ëª¨ë¸ í’ˆì§ˆ í‰ê°€")
    parser.add_argument("--video", type=str, required=True, help="ì…ë ¥ ì˜ìƒ íŒŒì¼")
    parser.add_argument("--models", nargs="+", required=True, help="í‰ê°€í•  ëª¨ë¸ ëª©ë¡")
    parser.add_argument("--device", type=str, default="auto", help="ë””ë°”ì´ìŠ¤ (auto/cuda/cpu)")
    parser.add_argument("--output-dir", type=str, default="result", help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬")
    
    args = parser.parse_args()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)
    
    # í‰ê°€ ì‹¤í–‰
    evaluator = UnsupervisedEvaluator(args.models, device=args.device)
    
    # ì„¸ê·¸ë¨¼íŠ¸ ë¡œë“œ
    segments = evaluator.load_segments(args.video)
    
    # ëª¨ë“  ëª¨ë¸ë¡œ ì˜ˆì¸¡
    evaluator.predict_all_models(segments, args.video)
    
    # í‰ê°€
    ranked = evaluator.evaluate()
    
    # ê²°ê³¼ ì¶œë ¥
    evaluator.print_results(ranked)
    
    # ê²°ê³¼ ì €ì¥
    video_name = Path(args.video).stem
    results_file = output_dir / f"unsupervised_eval_{video_name}.json"
    evaluator.save_results(ranked, results_file)
    
    # ì‹œê°í™”
    viz_file = output_dir / f"unsupervised_eval_{video_name}.png"
    evaluator.visualize(ranked, viz_file)
    
    print("\nâœ… Evaluation complete!")


if __name__ == "__main__":
    main()
