#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ìë™ í›„ë³´ íƒìƒ‰ê¸°: ë¼ë²¨ì´ ìˆëŠ” ì—¬ëŸ¬ ì˜ìƒì— ëŒ€í•´ ë‹¤ìˆ˜ì˜ ì˜¤ë””ì˜¤ ê°ì • ëª¨ë¸ì„ í‰ê°€í•˜ê³ ,
í‰ê·  ì •í™•ë„ê°€ ê°€ì¥ ë†’ì€ ëª¨ë¸ì„ ìë™ìœ¼ë¡œ ì¶”ì²œ/ì„ ì •í•©ë‹ˆë‹¤.

ì‚¬ìš© ì˜ˆì‹œ (PowerShell):
  .\venv\Scripts\python.exe tools\auto_select_best_model.py --device auto

ì˜µì…˜:
  --models ...         : íŠ¹ì • ëª¨ë¸ë§Œ ì§€ì •í•´ì„œ í‰ê°€ (ìƒëµ ì‹œ configì˜ audio_candidates + ì¼ë¶€ ì¶”ê°€ í›„ë³´)
  --disable-text       : í…ìŠ¤íŠ¸ ê°ì • ëª¨ë¸ ë¹„í™œì„±í™” (ê¸°ë³¸ê°’: ì‚¬ìš©)  â† ì˜¤ë””ì˜¤ ëª¨ë¸ ê³ ìœ  ì„±ëŠ¥ì„ ë³´ë ¤ë©´ ì¼œì„¸ìš”
  --batch-size N       : ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ 4)
  --cache-dir .cache   : ìºì‹œ ë””ë ‰í† ë¦¬
  --device auto|cuda|cpu
  --apply-best         : ìµœê³  ëª¨ë¸ì„ config.pyì˜ models.audioì— ì ìš©

ì¶œë ¥:
  result/auto_select_results.json
  result/auto_select_results.csv
  í„°ë¯¸ë„ ë¡œê·¸ì— ìˆœìœ„í‘œ ë° ì¶”ì²œ ëª¨ë¸ ì¶œë ¥
"""
from __future__ import annotations

import argparse
import json
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
import sys

ROOT = Path(__file__).resolve().parent.parent
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

from config import config as app_config  # type: ignore

# model_evaluatorì˜ ë‚´ë¶€ í•¨ìˆ˜ë¥¼ ì¬ì‚¬ìš© (ì •í™•ë„/ë©”íŠ¸ë¦­ ê³„ì‚°, ì¡°í•© í‰ê°€ ë“±)
from tools import model_evaluator as ME  # type: ignore


@dataclass
class Dataset:
    name: str
    video: Path
    labels: Path


def discover_datasets() -> List[Dataset]:
    """assets/*.mp4ì™€ labels/*_labels.jsonl, labelled_simpson.jsonlë¥¼ ë§¤ì¹­í•´ í‰ê°€ ì„¸íŠ¸ë¥¼ êµ¬ì„±"""
    datasets: List[Dataset] = []

    assets = ROOT / "assets"
    labels_dir = ROOT / "labels"

    # ì´ë¦„ ê·œì¹™: labels/<name>_labels.jsonl â†” assets/<name>.mp4
    for lp in labels_dir.glob("*_labels.jsonl"):
        name = lp.name.replace("_labels.jsonl", "")
        vp = assets / f"{name}.mp4"
        if vp.exists():
            datasets.append(Dataset(name=name, video=vp, labels=lp))

    # Simpson íŠ¹ë¡€ (ë£¨íŠ¸ì— ì¡´ì¬)
    simpson_labels = ROOT / "labelled_simpson.jsonl"
    simpson_video = assets / "simpson.mp4"
    if simpson_labels.exists() and simpson_video.exists():
        datasets.append(Dataset(name="simpson", video=simpson_video, labels=simpson_labels))

    # ì¤‘ë³µ ì œê±° (ì´ë¦„ ê¸°ì¤€)
    uniq: Dict[str, Dataset] = {d.name: d for d in datasets}
    return list(uniq.values())


def load_candidates(extra: Optional[List[str]] = None) -> List[str]:
    """configì˜ audio_candidatesì— ê¸°ë³¸ í›„ë³´ + ì¶”ê°€ í›„ë³´ë¥¼ ë³‘í•© í›„ ì¤‘ë³µ ì œê±°"""
    cfg_list: List[str] = app_config.get("models", "audio_candidates", default=[]) or []

    # ê³¼ê±°/ìš”ì²­ ê¸°ë°˜ ì¶”ê°€ í›„ë³´ (ì¤‘ë³µ ìë™ ì œê±°)
    extras = [
        # ì‚¬ìš©ì ìš”êµ¬ ì¬í™•ì¸ í›„ë³´ë“¤
        "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition",
        "superb/wav2vec2-large-superb-er",
    ]
    if extra:
        extras.extend(extra)

    merged = []
    seen = set()
    for m in cfg_list + extras:
        if not m:
            continue
        if m not in seen:
            merged.append(m)
            seen.add(m)
    return merged


def evaluate_audio_model_on_dataset(audio_model: str, ds: Dataset, device: str, batch_size: int, cache_dir: Path) -> Dict[str, Any]:
    records = ME.load_labels(ds.labels)
    result = ME.evaluate_combination(
        video_path=ds.video,
        records=records,
        audio_model=audio_model,
        text_model=None,  # ì˜¤ë””ì˜¤ ë‹¨ë… ì •í™•ë„
        device=device,
        batch_size=batch_size,
        cache_dir=cache_dir,
    )
    metrics = result["metrics"]

    # ë¹„ì¤‘ë¦½ ì •í™•ë„ ê³„ì‚° (ë¼ë²¨ì´ non-neutralì¸ ìƒ˜í”Œì— ëŒ€í•´ì„œë§Œ ì •í™•ë„)
    non_neutral_total = 0
    non_neutral_correct = 0
    cm = metrics.get("confusion", {})
    # ME.EMOTIONS ìˆœíšŒ
    for true_label in getattr(ME, "EMOTIONS", ("neutral","happy","sad","angry","fear","surprise","disgust")):
        if true_label == "neutral":
            continue
        row = cm.get(true_label, {})
        row_total = sum(row.values()) if isinstance(row, dict) else 0
        non_neutral_total += row_total
        non_neutral_correct += row.get(true_label, 0) if isinstance(row, dict) else 0
    non_neutral_acc = (non_neutral_correct / non_neutral_total) if non_neutral_total > 0 else None

    return {
        "dataset": ds.name,
        "samples": len(records),
        "accuracy": metrics["accuracy"],
        "macro_f1": metrics["macro_f1"],
        "neutral_rate": metrics["neutral_rate"],
        "non_neutral_accuracy": non_neutral_acc,
    }


def aggregate_scores(per_dataset: List[Dict[str, Any]]) -> Dict[str, float]:
    """ë°ì´í„°ì…‹ í¬ê¸°(ìƒ˜í”Œ ìˆ˜)ë¡œ ê°€ì¤‘ í‰ê· ì„ ê³„ì‚°"""
    total = sum(item["samples"] for item in per_dataset) or 1
    def wavg(key: str) -> float:
        return sum(item[key] * item["samples"] for item in per_dataset) / total
    # non-neutralì€ Noneì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìœ íš¨ê°’ë§Œ ê°€ì¤‘ í‰ê· 
    nn_vals = [(item["non_neutral_accuracy"], item["samples"]) for item in per_dataset if item.get("non_neutral_accuracy") is not None]
    if nn_vals:
        nn_total = sum(n for _, n in nn_vals)
        nn_avg = sum(v * n for v, n in nn_vals) / nn_total
    else:
        nn_avg = float("nan")
    return {
        "avg_accuracy": wavg("accuracy"),
        "avg_macro_f1": wavg("macro_f1"),
        "avg_neutral_rate": wavg("neutral_rate"),
        "avg_non_neutral_accuracy": nn_avg,
        "total_samples": total,
    }


def main() -> None:
    ap = argparse.ArgumentParser(description="ì—¬ëŸ¬ ë¼ë²¨ëœ ì˜ìƒì— ëŒ€í•´ ì˜¤ë””ì˜¤ ê°ì • ëª¨ë¸ ìë™ ë²¤ì¹˜ë§ˆí¬")
    ap.add_argument("--models", nargs="*", default=None, help="í‰ê°€í•  ì˜¤ë””ì˜¤ ëª¨ë¸ë“¤ (ìƒëµ ì‹œ ìë™)")
    ap.add_argument("--device", default="auto", help="auto|cuda|cpu")
    ap.add_argument("--batch-size", type=int, default=4)
    ap.add_argument("--cache-dir", type=Path, default=Path(".cache"))
    ap.add_argument("--apply-best", action="store_true", help="ìµœê³  ëª¨ë¸ì„ config.pyì— ì ìš©")
    args = ap.parse_args()

    # ì¥ì¹˜ ìë™ ê²°ì •
    device = ME.auto_device(args.device)
    cache_dir = args.cache_dir
    cache_dir.mkdir(parents=True, exist_ok=True)

    # ë°ì´í„°ì…‹ ê²€ìƒ‰
    datasets = discover_datasets()
    if not datasets:
        print("[ERR] í‰ê°€ ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. labels/*_labels.jsonl ë˜ëŠ” labelled_simpson.jsonlì„ í™•ì¸í•˜ì„¸ìš”.")
        sys.exit(2)

    print(f"\nğŸ“€ Datasets: {len(datasets)}ê°œ")
    for d in datasets:
        print(f" - {d.name}: video={d.video.name}, labels={d.labels.name}")

    # í›„ë³´ ëª¨ë¸ ìˆ˜ì§‘
    candidates = args.models or load_candidates()
    print(f"\nğŸ§ª Candidates: {len(candidates)}ê°œ")
    for m in candidates:
        print(f" - {m}")

    # í‰ê°€ ë£¨í”„
    all_results: List[Dict[str, Any]] = []
    ranking: List[Tuple[str, Dict[str, float]]] = []

    for idx, model in enumerate(candidates, start=1):
        print("\n" + "=" * 80)
        print(f"[{idx}/{len(candidates)}] Evaluating model: {model}")
        per_dataset: List[Dict[str, Any]] = []
        for ds in datasets:
            try:
                r = evaluate_audio_model_on_dataset(
                    audio_model=model,
                    ds=ds,
                    device=device,
                    batch_size=args.batch_size,
                    cache_dir=cache_dir,
                )
                per_dataset.append(r)
                nn = r.get("non_neutral_accuracy")
                nn_str = f"{nn:.3f}" if nn is not None else "n/a"
                print(" - {name:<8} | acc={acc:.3f} | f1={f1:.3f} | neu={neu:.3f} | nn_acc={nn_acc} | n={n}".format(
                    name=ds.name,
                    acc=r["accuracy"],
                    f1=r["macro_f1"],
                    neu=r["neutral_rate"],
                    nn_acc=nn_str,
                    n=r["samples"],
                ))
            except Exception as exc:
                print(f"   [WARN] {ds.name} ì‹¤íŒ¨: {exc}")
        if not per_dataset:
            print("   [SKIP] ëª¨ë“  ë°ì´í„°ì…‹ì—ì„œ ì‹¤íŒ¨")
            continue

        agg = aggregate_scores(per_dataset)
        nn_avg = agg.get("avg_non_neutral_accuracy")
        nn_avg_str = f"{nn_avg:.3f}" if nn_avg == nn_avg else "n/a"  # NaN ì²´í¬
        print(" -> AVG | acc={acc:.3f} | f1={f1:.3f} | neu={neu:.3f} | nn_acc={nn} | total={n}".format(
            acc=agg["avg_accuracy"], f1=agg["avg_macro_f1"], neu=agg["avg_neutral_rate"], nn=nn_avg_str, n=agg["total_samples"],
        ))

        all_results.append({
            "model": model,
            "per_dataset": per_dataset,
            "aggregate": agg,
        })
        ranking.append((model, agg))

    if not ranking:
        print("[ERR] ìœ íš¨í•œ í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(3)

    # ì •ë ¬: ë¹„ì¤‘ë¦½ ì •í™•ë„ ìµœìš°ì„ (ìœ íš¨ê°’ë§Œ), ê·¸ ë‹¤ìŒ ì „ì²´ ì •í™•ë„, ê·¸ ë‹¤ìŒ Macro F1
    def rank_key(item: Tuple[str, Dict[str, float]]):
        agg = item[1]
        nn = agg.get("avg_non_neutral_accuracy")
        nn_val = (-1.0 if nn != nn else nn)  # NaNì´ë©´ -1ë¡œ ì·¨ê¸‰ (ê¼´ì°Œ)
        return (nn_val, agg.get("avg_accuracy", 0.0), agg.get("avg_macro_f1", 0.0))
    ranking.sort(key=rank_key, reverse=True)

    print("\n" + "=" * 80)
    print("ğŸ† Overall Ranking (weighted by samples)")
    print("{:<3} {:<55} {:>8} {:>8} {:>8} {:>8}".format("#", "model", "acc", "f1", "neu", "nn_acc"))
    print("-" * 84)
    for i, (m, agg) in enumerate(ranking, start=1):
        nn_avg = agg.get("avg_non_neutral_accuracy")
        nn_avg_str = f"{nn_avg:.3f}" if nn_avg == nn_avg else "n/a"
        print("{:<3} {:<55} {:>8.3f} {:>8.3f} {:>8.3f} {:>8}".format(
            i, m[:55], agg["avg_accuracy"], agg["avg_macro_f1"], agg["avg_neutral_rate"], nn_avg_str
        ))

    best_model, best_scores = ranking[0]
    print("\nBest model:")
    print(json.dumps({"model": best_model, **best_scores}, indent=2, ensure_ascii=False))

    # ê²°ê³¼ ì €ì¥
    out_dir = ROOT / "result"
    out_dir.mkdir(exist_ok=True)
    (out_dir / "auto_select_results.json").write_text(
        json.dumps(all_results, indent=2, ensure_ascii=False), encoding="utf-8"
    )
    # ê°„ë‹¨ CSVë„ ì €ì¥
    try:
        import csv
        with (out_dir / "auto_select_results.csv").open("w", newline="", encoding="utf-8-sig") as f:
            w = csv.writer(f)
            w.writerow(["model", "avg_acc", "avg_f1", "avg_neu", "total_samples"])
            for m, agg in ranking:
                w.writerow([m, agg["avg_accuracy"], agg["avg_macro_f1"], agg["avg_neutral_rate"], agg["total_samples"]])
    except Exception:
        pass

    # ì„ íƒ ì ìš© (ì˜µì…˜)
    if args.apply_best:
        # config.pyë¥¼ ì§ì ‘ ìˆ˜ì •í•˜ì§€ ì•Šê³  ì‚¬ìš©ìì—ê²Œ ì¶”ì²œë§Œ í•  ìˆ˜ë„ ìˆì§€ë§Œ, í”Œë˜ê·¸ê°€ ìˆìœ¼ë©´ ì ìš©
        try:
            cfg_path = ROOT / "config.py"
            txt = cfg_path.read_text(encoding="utf-8")
            # 'audio': '<...>' ê°’ì„ ì¹˜í™˜ (ê°„ë‹¨í•œ ë°©ë²•)
            import re
            new_txt, n = re.subn(r"('audio'\s*:\s*)'[^']+'", rf"\1'{best_model}'", txt, count=1)
            if n == 0:
                print("[WARN] config.pyì—ì„œ 'audio' í•­ëª©ì„ ì°¾ì§€ ëª»í•´ ì¶”ê°€ëŠ” ìƒëµí•©ë‹ˆë‹¤.")
            else:
                cfg_path.write_text(new_txt, encoding="utf-8")
                print(f"[APPLIED] config.models.audio = {best_model}")
        except Exception as exc:
            print(f"[WARN] config ì ìš© ì‹¤íŒ¨: {exc}")


if __name__ == "__main__":
    main()
