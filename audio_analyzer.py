#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import torch
import numpy as np
import librosa
import math
from collections import defaultdict
from config import config

class AudioAnalyzer:
    """ì˜¤ë””ì˜¤ ë¶„ì„ ë° íŠ¹ì„± ì¶”ì¶œ í´ë˜ìŠ¤"""

    def __init__(self, sample_rate=16000):
        self.sample_rate = sample_rate
        # ì „ì—­ í†µê³„/ì„ê³„ ì €ì¥ êµ¬ì¡° ì´ˆê¸°í™”
        self.volume_stats = {
            'values': [],
            'mean': None,
            'std': None,
            'percentiles': {},
            'thresholds': {'soft': None, 'normal': None, 'loud': None}
        }
        self.pitch_stats = {
            'values': [],
            'p10': None,
            'p90': None
        }
        self.speech_rate_stats = {
            'values': [],
            'mean': None,
            'std': None,
            'thresholds': {'slow': None, 'normal': None, 'fast': None}
        }
        # ë°œí™” ì†ë„ë³„ ìê°„ ê°„ê²© (ìë§‰ ìŠ¤íƒ€ì¼ìš©)
        self.speech_rate_spacing = {'slow': 10, 'normal': 0, 'fast': -5}
        # ìµœì†Œ/ì´ìƒì  ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´
        self.min_duration = 0.2
        self.ideal_duration = 0.5
    def analyze_audio_features(self, segments, audio):
        """ì˜¤ë””ì˜¤ íŠ¹ì„± ë¶„ì„ (ìµœì í™” ë²„ì „)

        - ë³¼ë¥¨/í”¼ì¹˜/ë°œí™”ì†ë„ ì „ì—­ ë¶„í¬ ë¶„ì„
        - ì„¸ê·¸ë¨¼íŠ¸ ë‹¨ìœ„ RMS + íˆìŠ¤í…Œë¦¬ì‹œìŠ¤
        - ì„¸ê·¸ë¨¼íŠ¸ë‹¹ 1íšŒ pitch track ê³„ì‚° í›„ ë‹¨ì–´ êµ¬ê°„ ì¶”ì¶œ
        - ë‹¨ì–´ RMS ìºì‹± / ì¤‘ë³µ ê³„ì‚° ì œê±°
        """
        self.analyze_volume_distribution(audio)
        # í”¼ì¹˜ ë¶„í¬ëŠ” ì•„ë˜ ì„¸ê·¸ë¨¼íŠ¸ ë£¨í”„ì—ì„œ í•œ ë²ˆ ê³„ì‚°í•œ YINì„ ì¬í™œìš©í•˜ì—¬ ì§‘ê³„
        self.analyze_speech_rate_distribution(segments)

        # config ì¡°íšŒ ìºì‹œ (ë£¨í”„ ë‚´ ë°˜ë³µ í˜¸ì¶œ ìµœì†Œí™”)
        fast_mode = bool(config.get('analysis', 'fast_mode', default=False))

        prev_volume_level = None
        hop_length_seg = 256
        # ì„¸ê·¸ë¨¼íŠ¸ë³„ í‰ê·  í”¼ì¹˜ ëˆ„ì (ì¤‘ë³µ YIN ì œê±°)
        _pitch_values = []
        for segment in segments:
            duration = segment['end'] - segment['start']
            if duration < self.min_duration:
                seg_audio = self._get_context_audio(audio, segment, pad_duration=0.1)
            else:
                start_idx = int(segment['start'] * self.sample_rate)
                end_idx = int(segment['end'] * self.sample_rate)
                seg_audio = audio[start_idx:end_idx]

            if len(seg_audio) == 0:
                segment['volume_level'] = 'normal'
                segment['volume_stats'] = {'mean': 0.0, 'levels': ['normal']}
                segment['pitch_stats'] = {'levels': ['normal']}
                segment['speech_rate_stats'] = {'levels': ['normal']}
                continue

            seg_rms = self.compute_rms(seg_audio)
            vol_level_raw = self._classify_volume([seg_rms])[0]
            if prev_volume_level and prev_volume_level != vol_level_raw:
                thr = self.volume_stats['thresholds']
                soft_thr = thr.get('soft') or 0
                loud_thr = thr.get('normal') or 0
                margin_soft = soft_thr * 0.1 if soft_thr else 0.01
                margin_loud = loud_thr * 0.1 if loud_thr else 0.03
                if vol_level_raw == 'soft' and soft_thr and (seg_rms > soft_thr - margin_soft):
                    vol_level_raw = prev_volume_level
                elif vol_level_raw == 'loud' and loud_thr and (seg_rms < loud_thr + margin_loud):
                    vol_level_raw = prev_volume_level
            segment['volume_level'] = vol_level_raw
            prev_volume_level = vol_level_raw

            words = segment.get('words', [])
            # ì„¸ê·¸ë¨¼íŠ¸ pitch track 1íšŒ ê³„ì‚°
            frame_f0 = None
            frame_times = None
            try:
                # ì„¸ê·¸ë¨¼íŠ¸ë‹¹ 1íšŒë§Œ YIN ìˆ˜í–‰ (ë‹¨ì–´/ì„¸ê·¸ë¨¼íŠ¸ í”¼ì¹˜ ëª¨ë‘ ì´ ê°’ìœ¼ë¡œ ì²˜ë¦¬)
                frame_f0 = librosa.yin(seg_audio, fmin=50, fmax=600, hop_length=hop_length_seg)
                frame_times = (np.arange(len(frame_f0)) * hop_length_seg) / self.sample_rate
                # ì„¸ê·¸ë¨¼íŠ¸ í‰ê·  í”¼ì¹˜ ì§‘ê³„ (ìœ íš¨ í”„ë ˆì„ë§Œ)
                valid_f0 = frame_f0[frame_f0 > 0]
                if valid_f0.size > 0:
                    _pitch_values.append(float(np.mean(valid_f0)))
            except Exception:
                frame_f0 = None
                frame_times = None

            word_rms_values = []
            for word in words:
                try:
                    if not isinstance(word, dict) or 'start' not in word or 'end' not in word:
                        continue
                    w_start_idx = int(word['start'] * self.sample_rate)
                    w_end_idx = int(word['end'] * self.sample_rate)
                    if w_end_idx <= w_start_idx or w_end_idx > len(audio):
                        continue
                    w_audio = audio[w_start_idx:w_end_idx]
                    if len(w_audio) == 0:
                        word['volume_level'] = 'normal'
                        word['pitch_level'] = 'normal'
                        word['speech_rate'] = 'normal'
                        continue
                    # RMS ìºì‹œ
                    rms = self.compute_rms(w_audio)
                    word['rms'] = rms
                    word['volume_level'] = self._classify_volume([rms])[0]
                    # Pitch from segment track
                    if frame_f0 is not None and frame_times is not None:
                        rel_start = word['start'] - segment['start']
                        rel_end = word['end'] - segment['start']
                        mask = (frame_times >= rel_start) & (frame_times <= rel_end)
                        sel = frame_f0[mask]
                        sel = sel[sel > 0]
                        if sel.size > 0:
                            avg_pitch = np.mean(sel)
                            word['pitch_level'] = self.assign_pitch_level(avg_pitch)
                        else:
                            word['pitch_level'] = 'normal'
                    else:
                        word['pitch_level'] = 'normal'
                    # ë°œí™” ì†ë„ ê³„ì‚°: fast_modeì—ì„œëŠ” ê°„ì´ ê³„ì‚°ìœ¼ë¡œ CPU ì‚¬ìš© ì ˆê°
                    dur_w = word['end'] - word['start']
                    if dur_w > 0:
                        if fast_mode:
                            # ê°„ì´: ìŒì ˆ ë³µì¡ë„ / ì „ì²´ ë‹¨ì–´ ê¸¸ì´ ì‹œê°„
                            sr_val = self._estimate_syllable_complexity(word.get('word', '')) / max(dur_w, 1e-6)
                        else:
                            # ì •ë°€: ì‹¤ì œ ë°œì„± ì‹œê°„ ì¶”ì • ê¸°ë°˜
                            sr_val = self._calculate_phonetic_speech_rate(w_audio, word.get('word', ''), dur_w)
                        word['speech_rate'] = self.assign_speech_rate_level(float(sr_val))
                    else:
                        word['speech_rate'] = 'normal'
                    word_rms_values.append(rms)
                except Exception as e:
                    print(f"ë‹¨ì–´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    word['volume_level'] = 'normal'
                    word['pitch_level'] = 'normal'
                    word['speech_rate'] = 'normal'

            segment['volume_stats'] = {
                'mean': float(np.mean(word_rms_values)) if word_rms_values else seg_rms,
                'levels': [w.get('volume_level', 'normal') for w in words] if words else [vol_level_raw]
            }
            segment['pitch_stats'] = {
                'levels': [w.get('pitch_level', 'normal') for w in words] if words else ['normal']
            }
            segment['speech_rate_stats'] = {
                'levels': [w.get('speech_rate', 'normal') for w in words] if words else ['normal']
            }

        # ì„¸ê·¸ë¨¼íŠ¸ ë£¨í”„ê°€ ëë‚œ ë’¤ p10/p90 ê³„ì‚° (ì¤‘ë³µ YIN ì œê±°)
        if _pitch_values:
            self.pitch_stats['values'] = _pitch_values
            self.pitch_stats['p10'] = float(np.percentile(_pitch_values, 10))
            self.pitch_stats['p90'] = float(np.percentile(_pitch_values, 90))

        return segments

    def compute_voice_spans(self, segments):
        """ì„¸ê·¸ë¨¼íŠ¸ ë‚´ë¶€ í•˜ì´ë¸Œë¦¬ë“œ ìŒì„± íƒ€ì… ìŠ¤íŒ¬ ì‚°ì¶œ

        í•˜ì´ë¸Œë¦¬ë“œ ê·œì¹™(ì‚¬ì „ í•©ì˜):
        - ê¸°ë³¸ì€ ì„¸ê·¸ë¨¼íŠ¸ ë ˆë²¨ voice_type(whisper/normal/shout)ì„ ìœ ì§€
        - ë‹¨ì–´ë³„ RMS(ìƒëŒ€)ì™€ ì „ì—­ ë³¼ë¥¨ ë ˆë²¨(soft/loud)ì„ ì´ìš©í•´ ê°•í•œ ì§€ì—­ íŒ¨í„´ì´
          ì¼ì • ê¸¸ì´ ì´ìƒ ì§€ì†ë˜ë©´ ê·¸ êµ¬ê°„ë§Œ ë¶€ë¶„ ì¬íƒœê¹…
        - íˆìŠ¤í…Œë¦¬ì‹œìŠ¤: std ê¸°ë°˜ ìƒëŒ€ ì„ê³„ë¡œ í† ê¸€ ë°©ì§€, ìµœì†Œ ë‹¨ì–´ìˆ˜/ì§€ì†ì‹œê°„ ì ìš©
        - ê¸°ë³¸ê°’ ìš°ì„ : ì„¸ê·¸ë¨¼íŠ¸ê°€ whisperë©´ ì•½í•œ shout í›„ë³´ëŠ” ë¬´ì‹œ(ë°˜ëŒ€ë„ ë™ì¼)
        ê²°ê³¼ëŠ” segment['voice_spans']ì— ê¸°ë¡: [{'label', 'start_word', 'end_word'}]
        """
        MIN_SPAN_WORDS = 2
        MIN_DUR = {"whisper": 0.50, "shout": 0.40}
        REL_STD = 0.75

        for seg in segments:
            words = seg.get('words') or []
            if not words:
                # ë‹¨ì–´ê°€ ì—†ìœ¼ë©´ ì„¸ê·¸ë¨¼íŠ¸ ì „ì—­ ë¼ë²¨ë§Œ ìœ ì§€
                seg['voice_spans'] = [{'label': seg.get('voice_type', 'normal'), 'start_word': 0, 'end_word': -1}]
                continue

            # ë‹¨ì–´ RMS ìˆ˜ì§‘ (ë¯¸ì¡´ì¬ ì‹œ 0)
            rms_vals = []
            for w in words:
                r = float(w.get('rms', 0.0))
                rms_vals.append(r)
            arr = np.asarray(rms_vals, dtype=np.float32)
            seg_mean = float(arr.mean()) if arr.size > 0 else 0.0
            seg_std = float(arr.std()) if arr.size > 0 else 0.0
            if seg_std < 1e-6:
                seg_std = 1.0  # ìƒëŒ€ ì„ê³„ê°€ ë¬´ì˜ë¯¸í•´ì§€ëŠ” ê²ƒì„ ë°©ì§€

            # ë‹¨ì–´ë³„ í›„ë³´ ë¼ë²¨ ì‚°ì¶œ
            cand = [None] * len(words)
            for i, w in enumerate(words):
                r = float(w.get('rms', 0.0))
                vol_lvl = w.get('volume_level', 'normal')
                is_shout = (r >= seg_mean + REL_STD * seg_std) or (vol_lvl == 'loud')
                is_whisp = (r <= seg_mean - REL_STD * seg_std) or (vol_lvl == 'soft')
                if is_shout and not is_whisp:
                    cand[i] = 'shout'
                elif is_whisp and not is_shout:
                    cand[i] = 'whisper'
                else:
                    cand[i] = None

            # ì—°ì† ê·¸ë£¹í™” ë° ìµœì†Œ ê¸¸ì´/ì§€ì†ì‹œê°„ í•„í„°
            accepted_overrides = []  # (label, start_idx, end_idx)
            i = 0
            while i < len(words):
                if cand[i] is None:
                    i += 1
                    continue
                j = i
                lab = cand[i]
                while j + 1 < len(words) and cand[j + 1] == lab:
                    j += 1
                # i..j ê·¸ë£¹ ì§€ì†ì‹œê°„ ê³„ì‚°
                start_t = float(words[i].get('start', seg.get('start', 0.0)))
                end_t = float(words[j].get('end', seg.get('end', start_t)))
                dur = max(0.0, end_t - start_t)
                enough_words = (j - i + 1) >= MIN_SPAN_WORDS
                enough_dur = dur >= MIN_DUR.get(lab, 0.45)

                # ì„¸ê·¸ë¨¼íŠ¸ ì „ì—­ ë¼ë²¨ ëŒ€ë¹„ ì•½í•œ ë°˜ëŒ€ í›„ë³´ ì–µì œ
                seg_label = seg.get('voice_type', 'normal')
                if seg_label == 'whisper' and lab == 'shout':
                    # ë” ì—„ê²©: ë‹¨ì–´ìˆ˜ê°€ 3 ì´ìƒ + 0.5s ì´ìƒ
                    enough_words = (j - i + 1) >= max(3, MIN_SPAN_WORDS)
                    enough_dur = dur >= max(0.50, MIN_DUR['shout'])
                elif seg_label == 'shout' and lab == 'whisper':
                    enough_words = (j - i + 1) >= max(3, MIN_SPAN_WORDS)
                    enough_dur = dur >= max(0.55, MIN_DUR['whisper'])

                if enough_words and enough_dur:
                    accepted_overrides.append((lab, i, j))
                i = j + 1

            # ë‹¨ì–´ë³„ ìµœì¢… ë¼ë²¨ ì ìš©(ê¸°ë³¸: ì„¸ê·¸ë¨¼íŠ¸ ë¼ë²¨)
            base = seg.get('voice_type', 'normal')
            final_labels = [base] * len(words)
            for lab, si, sj in accepted_overrides:
                for k in range(si, sj + 1):
                    final_labels[k] = lab

            # ë™ì¼ ë¼ë²¨ ì—°ì† êµ¬ê°„ì„ voice_spansë¡œ ì••ì¶•
            spans = []
            cur_lab = final_labels[0]
            span_start = 0
            for idx in range(1, len(words)):
                if final_labels[idx] != cur_lab:
                    spans.append({
                        'label': cur_lab,
                        'start_word': span_start,
                        'end_word': idx - 1
                    })
                    cur_lab = final_labels[idx]
                    span_start = idx
            # ê¼¬ë¦¬ ì²˜ë¦¬
            spans.append({'label': cur_lab, 'start_word': span_start, 'end_word': len(words) - 1})

            seg['voice_spans'] = spans

        return segments

    def classify_voice_types(self, segments, audio):
        """ë°œí™”ì ì •ê·œí™” + ì˜¤ë””ì˜¤ ì‹ í˜¸ ê¸°ë°˜ Whisper/Shout ë¶„ë¥˜(ì¼ë°˜í™” ì•Œê³ ë¦¬ì¦˜)

        - í…ìŠ¤íŠ¸/íŠ¹ì • ì˜ìƒ íŠ¹í™” ë¡œì§ ì—†ì´, ì˜¤ë””ì˜¤ íŠ¹ì„±ë§Œìœ¼ë¡œ íŒì •
        - í™”ìë³„ ì •ê·œí™”(RMS/crest/HF/LF/í‹¸íŠ¸/í˜¸ê¸°ì„±)
        - ì ì‘í˜• ì„ê³„ê°’(í‰ê· +í‘œì¤€í¸ì°¨)ê³¼ ê°„ë‹¨í•œ ê°€ë“œë¡œ ì˜¤íƒ ì–µì œ
        """
        MIN_DUR = 0.30
        FRAME_MS = 0.025  # 25ms í”„ë ˆì„
        HOP_MS = 0.010    # 10ms í™‰
        # ìœ„ì¹˜ íŠ¹í™” íœ´ë¦¬ìŠ¤í‹± ì‚¬ìš© ì—¬ë¶€(ê¸°ë³¸ False)
        use_peak_pos = bool(config.get('voice', 'use_peak_position_heuristics', default=False))

        def crest_factor(x: np.ndarray) -> float:
            if len(x) == 0:
                return 0.0
            rms_v = self.compute_rms(x)
            if rms_v == 0:
                return 0.0
            return float(np.max(np.abs(x)) / (rms_v + 1e-9))

        def hf_lf_ratio(x: np.ndarray) -> float:
            try:
                if len(x) > self.sample_rate:
                    mid = len(x) // 2
                    half = self.sample_rate // 2
                    x = x[mid-half:mid+half]
                window = np.hanning(len(x))
                spec = np.abs(np.fft.rfft(x * window))
                freqs = np.fft.rfftfreq(len(x), d=1.0/self.sample_rate)
                lf_energy = spec[freqs <= 1000].sum() + 1e-9
                hf_mask = (freqs >= 2000) & (freqs <= 5000)
                hf_energy = spec[hf_mask].sum() + 1e-9
                return float(hf_energy / lf_energy)
            except Exception:
                return 1.0

        def spectral_tilt_analysis(x: np.ndarray) -> float:
            try:
                if len(x) < 512:
                    return 0.0
                window = np.hanning(len(x))
                spec = np.abs(np.fft.rfft(x * window))
                freqs = np.fft.rfftfreq(len(x), d=1.0/self.sample_rate)
                hf_mask = (freqs >= 2000) & (freqs <= 4000)
                lf_mask = (freqs >= 300) & (freqs <= 1500)
                if not (hf_mask.any() and lf_mask.any()):
                    return 0.0
                hf_energy = spec[hf_mask].mean()
                lf_energy = spec[lf_mask].mean() + 1e-9
                spectral_tilt = hf_energy / lf_energy
                return float(np.clip((spectral_tilt - 0.3) / 0.7, 0.0, 1.0))
            except Exception:
                return 0.0

        def breathiness_detection(x: np.ndarray) -> float:
            try:
                if len(x) < 512:
                    return 0.0
                window = np.hanning(len(x))
                spec = np.abs(np.fft.rfft(x * window))
                freqs = np.fft.rfftfreq(len(x), d=1.0/self.sample_rate)
                noise_mask = (freqs >= 6000) & (freqs <= 8000)
                if not noise_mask.any():
                    return 0.0
                noise_energy = spec[noise_mask].mean()
                total_energy = spec.mean() + 1e-9
                noise_ratio = noise_energy / total_energy
                return float(np.clip((noise_ratio - 0.02) / 0.05, 0.0, 1.0))
            except Exception:
                return 0.0

        segment_features = []
        speaker_buckets = defaultdict(lambda: {
            'rms': [],
            'crest': [],
            'hf_lf': [],
            'breathiness': [],
            'tilt': []
        })

        for idx, seg in enumerate(segments):
            start_idx = int(seg['start'] * self.sample_rate)
            end_idx = int(seg['end'] * self.sample_rate)
            if end_idx <= start_idx or end_idx > len(audio):
                continue
            duration = seg['end'] - seg['start']
            seg_audio = audio[start_idx:end_idx]
            if seg_audio.size == 0:
                continue

            rms = self.compute_rms(seg_audio)
            c_factor = crest_factor(seg_audio)
            ratio_hf_lf = hf_lf_ratio(seg_audio)
            spectral_tilt = spectral_tilt_analysis(seg_audio)
            breathiness = breathiness_detection(seg_audio)
            speaker_id = seg.get('speaker', 'Unknown')

            segment_features.append({
                'index': idx,
                'speaker': speaker_id,
                'rms': rms,
                'crest': c_factor,
                'hf_lf': ratio_hf_lf,
                'tilt': spectral_tilt,
                'breathiness': breathiness,
                'duration': duration
            })

            speaker_buckets[speaker_id]['rms'].append(rms)
            speaker_buckets[speaker_id]['crest'].append(c_factor)
            speaker_buckets[speaker_id]['hf_lf'].append(ratio_hf_lf)
            speaker_buckets[speaker_id]['breathiness'].append(breathiness)
            speaker_buckets[speaker_id]['tilt'].append(spectral_tilt)

        if not segment_features:
            for seg in segments:
                seg['voice_type'] = 'normal'
                seg['voice_type_confidence'] = 0.0
            return segments

        def mean_std(values):
            arr = np.asarray(values, dtype=np.float32)
            if arr.size == 0:
                return 0.0, 1.0
            mean = float(arr.mean())
            std = float(arr.std())
            if std < 1e-6:
                std = 1.0
            return mean, std

        speaker_stats = {}
        for speaker, feats in speaker_buckets.items():
            speaker_stats[speaker] = {
                'rms': mean_std(feats['rms']),
                'crest': mean_std(feats['crest']),
                'hf_lf': mean_std(feats['hf_lf']),
                'breathiness': mean_std(feats['breathiness']),
                'tilt': mean_std(feats['tilt'])
            }

        def normalize(value, mean, std):
            return (value - mean) / (std if std else 1.0)

        def sigmoid(x):
            return 1.0 / (1.0 + math.exp(-x))

        whisper_probs = []
        shout_probs = []

        # í”„ë ˆì„ ê¸°ë°˜ ì—ë„ˆì§€/ì§€ì†ì„± ë¶„ì„ í—¬í¼
        def _frame_rms(arr: np.ndarray) -> np.ndarray:
            """ë²¡í„°í™”ëœ í”„ë ˆì„ RMS ê³„ì‚° (librosa.feature.rms ì‚¬ìš©)."""
            frame_size = max(1, int(self.sample_rate * FRAME_MS))
            hop_size = max(1, int(self.sample_rate * HOP_MS))
            if arr.size == 0:
                return np.array([0.0], dtype=np.float32)
            # librosa.feature.rmsëŠ” 2D ë°˜í™˜(shape: 1, n_frames)
            rms2d = librosa.feature.rms(
                y=arr.astype(np.float32),
                frame_length=frame_size,
                hop_length=hop_size,
                center=False
            )
            return rms2d.flatten().astype(np.float32)

        # ê° ì„¸ê·¸ë¨¼íŠ¸ë³„ ì •ê·œí™”/í™•ë¥  ë° ì§€ì†ì‹œê°„ íŠ¹ì§• ê³„ì‚°
        for feat in segment_features:
            stats = speaker_stats.get(feat['speaker'])
            if not stats:
                stats = {
                    'rms': (0.02, 0.01),
                    'crest': (5.5, 0.5),
                    'hf_lf': (1.0, 0.2),
                    'breathiness': (0.1, 0.05),
                    'tilt': (0.2, 0.1)
                }

            rms_norm = normalize(feat['rms'], *stats['rms'])
            crest_norm = normalize(feat['crest'], *stats['crest'])
            hf_lf_norm = normalize(feat['hf_lf'], *stats['hf_lf'])
            breath_norm = normalize(feat['breathiness'], *stats['breathiness'])
            tilt_norm = normalize(feat['tilt'], *stats['tilt'])

            duration = feat['duration']
            duration_bonus = np.clip(duration - 0.45, -0.4, 0.6)
            long_bonus = max(0.0, duration - 0.35)

            whisper_logit = (
                (-2.0 * rms_norm) +
                (1.4 * feat['breathiness']) +
                (0.9 * max(0.0, hf_lf_norm)) +
                (-0.8 * crest_norm) +
                (0.4 * breath_norm) +
                (0.3 * duration_bonus)
            ) - 1.0

            shout_logit = (
                (2.0 * rms_norm) +
                (1.2 * feat['tilt']) +
                (0.9 * max(0.0, crest_norm)) +
                (-0.7 * hf_lf_norm) +
                (0.3 * tilt_norm) +
                (0.4 * long_bonus)
            ) - 0.9

            whisper_prob = sigmoid(whisper_logit)
            shout_prob = sigmoid(shout_logit)

            feat['whisper_prob'] = whisper_prob
            feat['shout_prob'] = shout_prob

            # === AV(Valence/Arousal) ê·¼ì‚¬ ê³„ì‚° ===
            # - Arousal: ì—ë„ˆì§€/ë°ê¸°/í”¼í¬ì„± ìƒê´€ (rms_norm, tilt, hf_lf_norm, crest_norm)
            # - Valence: í˜¸ê¸°ì„±(ì—­ìƒê´€), ê±°ì¹œ ê³ ì£¼íŒŒ/í”¼í¬(ì—­ìƒê´€) ê¸°ë°˜ ë³´ìˆ˜ì  ê·¼ì‚¬
            # ë²”ìœ„ë¥¼ ì¢íˆì§€ ì•Šë„ë¡ ì‘ì€ ê°€ì¤‘ìœ¼ë¡œ ì‹œê·¸ëª¨ì´ë“œ ë§¤í•‘
            arousal_logit = (
                1.6 * rms_norm +
                0.9 * feat['tilt'] +
                0.5 * max(0.0, hf_lf_norm) +
                0.4 * max(0.0, crest_norm)
            ) - 0.2 * max(0.0, breath_norm)
            arousal = float(np.clip(sigmoid(arousal_logit), 0.0, 1.0))

            valence_logit = (
                -1.3 * max(0.0, breath_norm) +
                -0.6 * max(0.0, hf_lf_norm) +
                -0.2 * max(0.0, crest_norm) +
                0.1
            )
            valence = float(np.clip(sigmoid(valence_logit), 0.0, 1.0))

            # === ì§€ì†ì„± ê¸°ë°˜ ê³ /ì € ì—ë„ˆì§€ ì‹œê°„ ì¶”ì • (shout/whisper ì•ˆì •í™”) ===
            seg = segments[feat['index']]
            start_idx = int(seg['start'] * self.sample_rate)
            end_idx = int(seg['end'] * self.sample_rate)
            seg_audio = audio[start_idx:end_idx]
            frame_rms = _frame_rms(seg_audio)
            spk_mean, spk_std = speaker_stats[feat['speaker']]['rms']
            high_thr = max(spk_mean + 0.50 * spk_std, 0.028)
            low_thr = min(spk_mean - 0.50 * spk_std, 0.016)
            frame_time = max(HOP_MS, FRAME_MS)  # ëŒ€ëµì  í”„ë ˆì„ ì‹œê°„ (ì´ˆ)
            high_frames = np.sum(frame_rms >= high_thr)
            low_frames = np.sum(frame_rms <= low_thr)
            high_dur = float(high_frames * frame_time)
            low_dur = float(low_frames * frame_time)
            total_dur = max(1e-6, feat['duration'])
            feat['high_energy_dur'] = high_dur
            feat['low_energy_dur'] = low_dur
            feat['high_energy_frac'] = float(np.clip(high_dur / total_dur, 0.0, 1.0))
            feat['low_energy_frac'] = float(np.clip(low_dur / total_dur, 0.0, 1.0))

            # ì•/ë’¤ êµ¬ê°„ì˜ ê³ ì—ë„ˆì§€ ë¶„í¬(ì´ˆë°˜ ê³¼ë„íƒì§€ ì–µì œ, ë§ë¯¸ ê°•ì¡° ê²€ì¦)
            n_frames = max(1, frame_rms.size)
            early_end = int(0.25 * n_frames)
            late_start = int(0.60 * n_frames)
            if early_end <= 0:
                early_end = min(1, n_frames)
            if late_start >= n_frames:
                late_start = max(0, n_frames - 1)
            early_high = int(np.sum(frame_rms[:early_end] >= high_thr)) if frame_rms.size > 0 else 0
            late_high = int(np.sum(frame_rms[late_start:] >= high_thr)) if frame_rms.size > 0 else 0
            feat['early_high_frac'] = float(np.clip((early_high * frame_time) / total_dur, 0.0, 1.0))
            feat['late_high_frac'] = float(np.clip((late_high * frame_time) / total_dur, 0.0, 1.0))

            # ë‹¨ì–´ í”¼í¬ ì •ë ¬: ì„¸ê·¸ë¨¼íŠ¸ ë‚´ ê°€ì¥ í° RMS ë‹¨ì–´ ìœ„ì¹˜(ìƒëŒ€ì‹œê°„)ì™€ RMS
            words = seg.get('words', []) or []
            peak_rms = -1.0
            peak_pos_rel = 0.0
            soft_word_frac = 0.0
            loud_word_frac = 0.0
            if words:
                wrms = []
                w_soft = 0
                w_loud = 0
                for w in words:
                    r = float(w.get('rms', 0.0)) if isinstance(w, dict) else 0.0
                    wrms.append(r)
                    # í™”ì ê¸°ì¤€ ìƒëŒ€ì  soft/loud ì¶”ì • (ë¶„ë¥˜ ì„ í–‰ ì—¬ë¶€ì™€ ë¬´ê´€í•˜ê²Œ ë³µì›ì  ê³„ì‚°)
                    if r > 0.0:
                        if r <= (spk_mean - 0.50 * spk_std):
                            w_soft += 1
                        elif r >= (spk_mean + 0.50 * spk_std):
                            w_loud += 1
                if any(v > 0 for v in wrms):
                    j = int(np.argmax(wrms))
                    w = words[j]
                    w_mid = 0.5 * (float(w.get('start', seg.get('start', 0))) + float(w.get('end', seg.get('end', 0))))
                    peak_rms = float(wrms[j])
                    # ì„¸ê·¸ë¨¼íŠ¸ ìƒëŒ€ ìœ„ì¹˜ 0~1
                    feat_start = float(seg.get('start', 0.0))
                    feat_end = float(seg.get('end', feat_start))
                    dur = max(1e-6, feat_end - feat_start)
                    peak_pos_rel = float(np.clip((w_mid - feat_start) / dur, 0.0, 1.0))
                soft_word_frac = float(w_soft / max(1, len(words)))
                loud_word_frac = float(w_loud / max(1, len(words)))
            feat['peak_word_rms'] = peak_rms
            feat['peak_pos_rel'] = peak_pos_rel
            feat['soft_word_frac'] = soft_word_frac
            feat['loud_word_frac'] = loud_word_frac

            whisper_probs.append(whisper_prob)
            shout_probs.append(shout_prob)

        whisper_probs_arr = np.array(whisper_probs)
        shout_probs_arr = np.array(shout_probs)

        base_whisper_thr = 0.60
        base_shout_thr = 0.62

        def adaptive_threshold(prob_arr, base):
            """ë³´ìˆ˜ì  ìƒí–¥ë§Œ í—ˆìš©í•˜ëŠ” ì ì‘í˜• ì„ê³„ê°’.
            - ë¶„í¬ê°€ ë‚®ì„ìˆ˜ë¡ ê³¼ë„í•˜ê²Œ ì˜¬ë¼ê°€ì§€ ì•Šë„ë¡ mean + 0.5*stdë§Œ ë°˜ì˜
            - ìƒí•œì„ ì¡°ê¸ˆ ë‚®ì¶° 0.88ë¡œ í´ë¦½ (ê³¼ë„ ë³´ìˆ˜í™” ë°©ì§€)
            - í‘œë³¸ì´ ì ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
            """
            if prob_arr.size < 4:
                return base
            mean = float(prob_arr.mean())
            std = float(prob_arr.std())
            candidate = mean + 0.5 * std
            return float(np.clip(max(base, candidate), base, 0.88))

        whisper_threshold = adaptive_threshold(whisper_probs_arr, base_whisper_thr)
        shout_threshold = adaptive_threshold(shout_probs_arr, base_shout_thr)

        # 1ì°¨ íŒì •
        preliminary_types = {}
        for feat in segment_features:
            idx = feat['index']
            whisper_prob = feat['whisper_prob']
            shout_prob = feat['shout_prob']

            speaker_mean, speaker_std = speaker_stats[feat['speaker']]['rms']
            crest_mean, crest_std = speaker_stats[feat['speaker']]['crest']
            hf_mean, hf_std = speaker_stats[feat['speaker']]['hf_lf']
            rms_guard_whisper = (feat['rms'] <= speaker_mean - 0.45 * speaker_std) or (feat['rms'] <= 0.016)
            rms_guard_shout = (feat['rms'] >= speaker_mean + 0.45 * speaker_std) or (feat['rms'] >= 0.028)
            breath_guard = feat['breathiness'] >= 0.45
            tilt_guard = feat['tilt'] >= 0.5
            crest_guard = (feat['crest'] >= crest_mean + 0.35 * crest_std) or (feat['crest'] >= 6.2)
            hf_guard = feat['hf_lf'] <= (
                speaker_stats[feat['speaker']]['hf_lf'][0] - 0.35 * speaker_stats[feat['speaker']]['hf_lf'][1]
            )
            hf_high_guard = feat['hf_lf'] >= (hf_mean + 0.10 * hf_std)

            # ë„ˆë¬´ ì§§ì€ êµ¬ê°„ì€ normal ìœ ì§€ (ì¡ìŒ ì–µì œ)
            seg = segments[idx]
            dur = seg.get('end', 0) - seg.get('start', 0)
            if dur < 0.30:
                voice_type = 'normal'
                confidence = max(0.0, max(whisper_prob, shout_prob) - 0.5)
            else:
                # ìµœì¢… íŒì •: ì˜¤ë””ì˜¤ ì‹ í˜¸ë§Œ ì‚¬ìš© + ì§€ì†ì„± ê°€ë“œ
                crest_upper = min(6.0, crest_mean + 0.15 * crest_std)
                sustained_low_ok = (feat.get('low_energy_dur', 0.0) >= 0.45) or (feat.get('low_energy_frac', 0.0) >= 0.60)
                sustained_high_ok = (feat.get('high_energy_dur', 0.0) >= 0.35) or (feat.get('high_energy_frac', 0.0) >= 0.40)
                # ì§§ì€ ê°íƒ„ì‚¬(ì˜ˆ: "No way!") ë³´ì •: ì¤‘ê°„ ê¸¸ì´(0.40~0.80s)ì—ì„œ ê³ ì—ë„ˆì§€ ë¹„ìœ¨ì´ 0.30 ì´ìƒì´ê³ 
                # ìŠ¤í™íŠ¸ëŸ´ ì§€í‘œ(tilt/crest)ê°€ ê°•í•˜ë©´ shout í—ˆìš©
                if not sustained_high_ok and 0.40 <= dur <= 0.80:
                    if (feat.get('high_energy_frac', 0.0) >= 0.30) and (tilt_guard or crest_guard):
                        sustained_high_ok = True

                # Whisper íŒì • ê°•í™”: soft ë‹¨ì–´ ë¹„ìœ¨/ì €ì—ë„ˆì§€ ì§€ì†ìœ¼ë¡œ ì¼ê´€ì„± í™•ë³´
                soft_majority = feat.get('soft_word_frac', 0.0) >= 0.70
                whisper_like = (
                    whisper_prob >= (whisper_threshold + 0.01)
                    or (soft_majority and whisper_prob >= (whisper_threshold - 0.03))
                )
                if (
                    whisper_like and
                    (whisper_prob - shout_prob) >= 0.12 and
                    rms_guard_whisper and
                    (feat['breathiness'] >= 0.50 or soft_majority) and
                    feat['crest'] <= crest_upper and
                    dur >= 0.50 and
                    sustained_low_ok
                ):
                    voice_type = 'whisper'
                    confidence = whisper_prob
                else:
                    # Shout íŒë‹¨: ê¸¸ì´ì— ë”°ë¥¸ ì¡°ê±´ ê°€ë³€í™”ë¡œ ê³¼ë„ ì–µì œ ì™„í™”
                    spectral_combo = (1 if tilt_guard else 0) + (1 if crest_guard else 0) + (1 if hf_guard else 0)
                    sustained_ok = sustained_high_ok
                    if dur >= 1.0:
                        sustained_ok = (feat.get('high_energy_frac', 0.0) >= 0.50) or (feat.get('high_energy_dur', 0.0) >= 0.60)
                    else:
                        # ì¤‘/ë‹¨ê¸¸ì´ êµ¬ê°„ì€ ì™„í™”ëœ ì§€ì†ì„± ê¸°ì¤€
                        sustained_ok = (feat.get('high_energy_frac', 0.0) >= 0.30) or (feat.get('high_energy_dur', 0.0) >= 0.30)

                    # ê¸°ë³¸ ê·œì¹™ (ê¸´ êµ¬ê°„: ìŠ¤í™íŠ¸ëŸ´ ê°€ë“œ 2ê°œ, ì§§ì€ êµ¬ê°„: 1ê°œë¡œ í—ˆìš©)
                    min_spectral = 2 if dur >= 0.90 else 1
                    # ëŠ¦ì€ í”¼í¬/ë§ë¯¸ ê³ ì—ë„ˆì§€ ê°€ì¤‘(ìœ„ì¹˜ íœ´ë¦¬ìŠ¤í‹± ë¹„í™œì„± ìœ ì§€í•˜ë˜, ìˆœìˆ˜ ì—ë„ˆì§€ ê¸°ë°˜)
                    late_emph_ok = feat.get('late_high_frac', 0.0) >= (0.12 if dur < 0.9 else 0.18)
                    base_shout_cond = (
                        shout_prob >= max(shout_threshold, 0.66) and
                        (shout_prob - whisper_prob) >= 0.10 and
                        rms_guard_shout and
                        spectral_combo >= min_spectral and
                        sustained_ok and
                        (late_emph_ok or use_peak_pos)
                    )

                    # ë³´ì¡° ê·œì¹™: ê°íƒ„ì‚¬í˜•(0.40~0.90s)ì—ì„œ ìŠ¤í™íŠ¸ëŸ´ ê°•í•˜ê³  ì—ë„ˆì§€ ë¹„ìœ¨ ì¶©ë¶„í•˜ë©´ ì•½ê°„ ë‚®ì€ ì„ê³„ë„ í—ˆìš©
                    exclaim_cond = (
                        0.40 <= dur <= 0.90 and
                        shout_prob >= (max(shout_threshold, 0.66) - 0.03) and
                        rms_guard_shout and
                        (tilt_guard or crest_guard) and
                        (feat.get('high_energy_frac', 0.0) >= 0.25) and
                        (feat.get('late_high_frac', 0.0) >= 0.12)
                    )

                    # ë‹¨ì–´ í”¼í¬ ì •ë ¬ ê¸°ë°˜ ë³´ì •(ì˜µì…˜): ê¸°ë³¸ ë¹„í™œì„±í™”ë¡œ ì¼ë°˜ì„± ìœ ì§€
                    peak_rel = feat.get('peak_pos_rel', 0.0)
                    end_peak_promote = False
                    early_peak_demote = False
                    if use_peak_pos:
                        # ëë¶€ë¶„(>=60%) í”¼í¬ + ê³ ì—ë„ˆì§€ë©´ ìŠ¹ê²© í—ˆìš©
                        end_peak_promote = (peak_rel >= 0.60) and (feat.get('high_energy_frac', 0.0) >= 0.30) and (tilt_guard or crest_guard)
                        # ì´ˆë°˜(<=25%) í”¼í¬ë§Œ ìˆê³  ê³ ì—ë„ˆì§€ ì§€ì†ì´ ì•½í•˜ë©´ ì–µì œ
                        early_peak_demote = (peak_rel <= 0.25) and (feat.get('high_energy_frac', 0.0) < 0.45) and (not sustained_ok)

                    # ì´ˆë°˜ ê³¼ë„íƒì§€ ì–µì œ: ì´ˆë°˜ë§Œ ë†’ê³  ë§ë¯¸ ì—ë„ˆì§€ê°€ ë¹ˆì•½í•˜ë©´ ì–µì œ
                    early_only = (
                        feat.get('early_high_frac', 0.0) >= 0.30 and
                        feat.get('late_high_frac', 0.0) < 0.08 and
                        not sustained_ok
                    )

                    if (base_shout_cond or exclaim_cond) and not early_only:
                        voice_type = 'shout'
                        confidence = shout_prob
                        if early_peak_demote:
                            voice_type = 'normal'
                            confidence = max(0.0, confidence - 0.1)
                    elif end_peak_promote and shout_prob >= (max(shout_threshold, 0.66) - 0.05) and rms_guard_shout:
                        voice_type = 'shout'
                        confidence = max(shout_prob, 0.66)
                    else:
                        voice_type = 'normal'
                        confidence = max(0.0, max(whisper_prob, shout_prob) - 0.5)

            preliminary_types[idx] = (voice_type, float(confidence))
            analysis = segments[idx].setdefault('voice_analysis', {})
            analysis.update({
                'rms': feat['rms'],
                'crest_factor': feat['crest'],
                'hf_lf_ratio': feat['hf_lf'],
                'spectral_tilt': feat['tilt'],
                'breathiness': feat['breathiness'],
                'whisper_prob': round(whisper_prob, 3),
                'shout_prob': round(shout_prob, 3),
                'speaker_rms_mean': speaker_stats[feat['speaker']]['rms'][0],
                'speaker_rms_std': speaker_stats[feat['speaker']]['rms'][1],
                'speaker_crest_mean': crest_mean,
                'speaker_crest_std': crest_std,
                'high_energy_frac': feat.get('high_energy_frac', 0.0),
                'low_energy_frac': feat.get('low_energy_frac', 0.0)
            })
            # AV ê·¼ì‚¬ê°’ì„ ì„¸ê·¸ë¨¼íŠ¸ì— ì €ì¥
            segments[idx]['av'] = {
                'arousal': float(np.clip(arousal, 0.0, 1.0)) if 'arousal' in locals() else None,
                'valence': float(np.clip(valence, 0.0, 1.0)) if 'valence' in locals() else None,
                'source': 'approx'
            }

        # 2ì°¨ ìŠ¤ë¬´ë”©: ë™ì¼ í™”ì ì¸ì ‘ ì„¸ê·¸ë¨¼íŠ¸ í™•ì‚°/ì–µì œ
        # - ê³ ë¦½ëœ ì§§ì€ shout ì–µì œ: dur<0.45ì´ê³  ì–‘ì˜†ì´ normalì´ë©° high_energy_frac<0.3 â†’ normal
        # - ì¸ì ‘ ì„¸ê·¸ë¨¼íŠ¸ í™•ì‚°: shoutì¸ ì„¸ê·¸ë¨¼íŠ¸ì˜ ì´ì›ƒ(Â±0.25s)ì´ ê°™ì€ í™”ìì´ê³  shout_prob ì¶©ë¶„í•˜ë©° high_energy_frac>=0.3 â†’ shoutë¡œ ìŠ¹ê²©
        index_to_feat = {f['index']: f for f in segment_features}
        for i, seg in enumerate(segments):
            if i not in preliminary_types:
                continue
            vtype, conf = preliminary_types[i]
            feat = index_to_feat.get(i)
            if not feat:
                continue
            dur = seg.get('end', 0) - seg.get('start', 0)
            spk = seg.get('speaker', 'Unknown')

            # ì´ì›ƒ ì¸ë±ìŠ¤ í›„ë³´
            prev_i = i - 1 if i - 1 >= 0 else None
            next_i = i + 1 if i + 1 < len(segments) else None
            neighbors = [j for j in [prev_i, next_i] if j is not None]

            # ì‹œê°„ ê°„ê²© ì²´í¬ (0.25s ì´ë‚´ë§Œ ì´ì›ƒìœ¼ë¡œ ê°„ì£¼)
            valid_neighbors = []
            for j in neighbors:
                if segments[j].get('speaker', 'Unknown') != spk:
                    continue
                gap = 0.0
                if j == prev_i:
                    gap = max(0.0, seg.get('start', 0) - segments[j].get('end', 0))
                else:
                    gap = max(0.0, segments[j].get('start', 0) - seg.get('end', 0))
                if gap <= 0.25:
                    valid_neighbors.append(j)

            # ì–µì œ ê·œì¹™: ê³ ë¦½ëœ ì§§ì€ shout â†’ normal
            if vtype == 'shout' and dur < 0.45:
                neighbor_types = [preliminary_types.get(j, ('normal', 0.0))[0] for j in valid_neighbors]
                high_frac = index_to_feat[i].get('high_energy_frac', 0.0)
                if all(nt != 'shout' for nt in neighbor_types) and high_frac < 0.30:
                    vtype = 'normal'
                    conf = max(0.0, conf - 0.1)

            # ì¶”ê°€ ì–µì œ: ì´ˆë°˜ë§Œ ê°•í•œ ì—ë„ˆì§€ë¡œ ì¸í•œ ì˜¤íƒ ë°©ì§€
            if vtype == 'shout':
                efrac = index_to_feat[i].get('early_high_frac', 0.0)
                lfrac = index_to_feat[i].get('late_high_frac', 0.0)
                if dur >= 0.35 and efrac >= 0.35 and lfrac < 0.08:
                    # ì´ì›ƒ ì¤‘ í•˜ë‚˜ê°€ shoutì´ê±°ë‚˜, ë³¸ì¸ lateê°€ ê½¤ ìˆìœ¼ë©´ ìœ ì§€
                    neighbor_is_shout = any(preliminary_types.get(j, ('normal', 0.0))[0] == 'shout' for j in valid_neighbors)
                    if not neighbor_is_shout:
                        vtype = 'normal'
                        conf = max(0.0, conf - 0.1)

            # í™•ì‚° ê·œì¹™: ì´ì›ƒ ìŠ¹ê²©
            if vtype == 'shout':
                for j in valid_neighbors:
                    ntype, nconf = preliminary_types.get(j, ('normal', 0.0))
                    nfeat = index_to_feat.get(j)
                    if not nfeat:
                        continue
                    if ntype != 'whisper' and nfeat.get('high_energy_frac', 0.0) >= 0.30:
                        # í™•ì‚° ì¡°ê±´: shout_probê°€ ì„ê³„ì¹˜ì— ê·¼ì ‘
                        if (nfeat['shout_prob'] >= (max(shout_threshold, 0.68) - 0.05)) and (nfeat.get('late_high_frac', 0.0) >= 0.10):
                            preliminary_types[j] = ('shout', max(nconf, nfeat['shout_prob']))

            # ìµœì¢… ê¸°ë¡
            preliminary_types[i] = (vtype, conf)

        # === ë³´ìˆ˜ì  ì„¸ê·¸ë¨¼íŠ¸ ë ˆë²¨ ê²Œì´íŒ… + ì¹´ìš´íŠ¸ ë³´ì • ===
        # 1) ì´ˆê¸° ì¹´ìš´íŠ¸ ê¸°ë¡
        orig_whisper = sum(1 for v, _ in preliminary_types.values() if v == 'whisper')
        orig_shout = sum(1 for v, _ in preliminary_types.values() if v == 'shout')

        # 2) ê²Œì´íŒ… ê¸°ì¤€ (ë³´ìˆ˜ì )
        strict_margin = {'whisper': 0.18, 'shout': 0.16}
        # ì—ë„ˆì§€ ì§€ì† ê¸°ì¤€(ê¸°ë³¸ê°’; ê¸¸ì´ì— ë”°ë¼ ìƒë‹¨ì—ì„œ ì´ë¯¸ ì¡°ì •ë¨)
        min_frac = {'whisper': 0.50, 'shout': 0.40}

        # í›„ë³´ ì €ì¥
        demoted_whisper = []  # (idx, whisper_prob, margin)
        demoted_shout = []    # (idx, shout_prob, margin)

        final_types = dict(preliminary_types)
        for idx, (vtype, conf) in list(final_types.items()):
            feat = index_to_feat.get(idx, {})
            wprob = float(feat.get('whisper_prob', 0.0))
            sprob = float(feat.get('shout_prob', 0.0))
            if vtype == 'whisper':
                margin = wprob - sprob
                low_frac = float(feat.get('low_energy_frac', 0.0))
                if not (wprob >= whisper_threshold and margin >= strict_margin['whisper'] and low_frac >= min_frac['whisper']):
                    final_types[idx] = ('normal', max(0.0, conf - 0.1))
                    demoted_whisper.append((idx, wprob, margin))
            elif vtype == 'shout':
                margin = sprob - wprob
                high_frac = float(feat.get('high_energy_frac', 0.0))
                if not (sprob >= max(shout_threshold, 0.66) and margin >= strict_margin['shout'] and high_frac >= min_frac['shout']):
                    final_types[idx] = ('normal', max(0.0, conf - 0.1))
                    demoted_shout.append((idx, sprob, margin))

        # 3) ì¹´ìš´íŠ¸ ë³´ì •: ì§€ë‚˜ì¹˜ê²Œ ì¤„ì–´ë“¤ë©´ ìƒìœ„ í›„ë³´ ì¼ë¶€ ë³µêµ¬ (ëª©í‘œ: ì›ë˜ì˜ 85%)
        def restore_top(demoted_list, target_count, cls):
            if target_count <= 0 or not demoted_list:
                return
            # ì ìˆ˜ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ë³µì› (í™•ë¥  ìš°ì„ , ë™ë¥  ì‹œ margin)
            key_idx = 1 if cls == 'whisper' else 1
            demoted_sorted = sorted(demoted_list, key=lambda x: (x[1], x[2]), reverse=True)
            restored = 0
            for idx, prob, margin in demoted_sorted:
                if restored >= target_count:
                    break
                # ì™„í™” ê¸°ì¤€ìœ¼ë¡œ ì¬í—ˆìš©
                final_types[idx] = (cls, prob)
                restored += 1

        new_whisper = sum(1 for v, _ in final_types.values() if v == 'whisper')
        new_shout = sum(1 for v, _ in final_types.values() if v == 'shout')
        target_whisper = max(0, int(orig_whisper * 0.85))
        target_shout = max(0, int(orig_shout * 0.85))
        if new_whisper < target_whisper:
            restore_top(demoted_whisper, target_whisper - new_whisper, 'whisper')
        if new_shout < target_shout:
            restore_top(demoted_shout, target_shout - new_shout, 'shout')

        # ê²°ê³¼ ë°˜ì˜ (ìµœì¢…)
        for idx, (vtype, conf) in final_types.items():
            segments[idx]['voice_type'] = vtype
            segments[idx]['voice_type_confidence'] = round(float(conf), 3)

        total_segments = len(segments)
        whisper_count = sum(1 for seg in segments if seg.get('voice_type') == 'whisper')
        shout_count = sum(1 for seg in segments if seg.get('voice_type') == 'shout')
        normal_count = total_segments - whisper_count - shout_count

        whisper_pct = (whisper_count / total_segments * 100) if total_segments > 0 else 0
        shout_pct = (shout_count / total_segments * 100) if total_segments > 0 else 0
        normal_pct = (normal_count / total_segments * 100) if total_segments > 0 else 0

        # ë¹„ìœ¨ í›„ì²˜ë¦¬ ì œê±°: ì¼ë°˜í™” ì•Œê³ ë¦¬ì¦˜ ìœ ì§€(ì˜ìƒ íŠ¹í™” ì–µì œ ë°©ì§€)

        try:
            print(f"ğŸ¯ Voice Type ë¶„ë¥˜ ê²°ê³¼ (ì´ {total_segments}ê°œ ì„¸ê·¸ë¨¼íŠ¸):")
            print(f"   Whisper: {whisper_count}ê°œ ({whisper_pct:.1f}%)")
            print(f"   Shout: {shout_count}ê°œ ({shout_pct:.1f}%)")
            print(f"   Normal: {normal_count}ê°œ ({normal_pct:.1f}%)")
            print(f"   ë™ì  ì„ê³„ê°’: whisper_thr~{whisper_threshold:.2f}, shout_thr~{shout_threshold:.2f}")
        except Exception:
            # Windows ë“± ì½˜ì†” ì¸ì½”ë”© ì´ìŠˆ ëŒ€ë¹„ (ì´ëª¨ì§€/ìœ ë‹ˆì½”ë“œ ì—†ì´ ì¶œë ¥)
            print(f"Voice Type Result (total {total_segments} segments):")
            print(f"   Whisper: {whisper_count} ({whisper_pct:.1f}%)")
            print(f"   Shout: {shout_count} ({shout_pct:.1f}%)")
            print(f"   Normal: {normal_count} ({normal_pct:.1f}%)")
            print(f"   Thr: whisper~{whisper_threshold:.2f}, shout~{shout_threshold:.2f}")

        self.voice_type_stats = {
            'mean_rms': float(np.mean([f['rms'] for f in segment_features])) if segment_features else 0.0,
            'std_rms': float(np.std([f['rms'] for f in segment_features])) if segment_features else 0.0,
            'whisper_threshold': whisper_threshold,
            'shout_threshold': shout_threshold,
            'whisper_pct': whisper_pct,
            'shout_pct': shout_pct,
            'normal_pct': normal_pct
        }
        return segments

    def compute_rms(self, audio_segment):
        """ì˜¤ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ì˜ RMS(Root Mean Square) ë³¼ë¥¨ ê³„ì‚°"""
        if isinstance(audio_segment, np.ndarray):
            audio_segment = torch.tensor(audio_segment, dtype=torch.float32)
        return torch.sqrt(torch.mean(audio_segment**2)).item()

    

    def _estimate_syllables(self, text):
        """í…ìŠ¤íŠ¸ì˜ ìŒì ˆ ìˆ˜ ì¶”ì • (í•œê¸€ ë° ì˜ì–´ ì§€ì›)"""
        count = 0
        for ch in text:
            if '\uAC00' <= ch <= '\uD7A3':
                count += 1
            elif ch.lower() in 'aeiouy':
                count += 1
        return max(1, count)

    # ì „ì—­ í†µê³„ëŠ” analyze_volume_distributionë¥¼ ì‚¬ìš©

    def analyze_volume_distribution(self, audio):
        """ì „ì²´ ì˜¤ë””ì˜¤ì˜ ë³¼ë¥¨ ë¶„í¬ ë¶„ì„ - ì ì‘í˜• ì„ê³„ê°’ ë°©ì‹"""
        # 1ì´ˆ ë‹¨ìœ„ë¡œ ì²­í¬ ë¶„í• í•˜ì—¬ RMS ë³¼ë¥¨ ê³„ì‚°
        chunk_size = self.sample_rate
        volumes = []
        
        for i in range(0, len(audio), chunk_size):
            chunk = audio[i:i+chunk_size]
            if len(chunk) > 0:
                rms = self.compute_rms(chunk)
                volumes.append(rms)

        if not volumes:
            return

        # ë³¼ë¥¨ ë¶„í¬ í†µê³„ ê³„ì‚°
        volumes = np.array(volumes)
        self.volume_stats['values'] = volumes
        self.volume_stats['mean'] = np.mean(volumes)
        self.volume_stats['std'] = np.std(volumes)
        
        # ë°±ë¶„ìœ„ìˆ˜ ê¸°ë°˜ ì´ˆê¸° ì„ê³„ê°’ (P30, P70ì˜ ì‹¤ì œ ê°’ì„ êµ¬í•¨)
        p30 = np.percentile(volumes, 30)
        p70 = np.percentile(volumes, 70)
        
        initial_thresholds = {
            'soft': p30,     # í•˜ìœ„ 30%ì— í•´ë‹¹í•˜ëŠ” ë³¼ë¥¨ ê°’
            'normal': p70,   # ìƒìœ„ 30%ì— í•´ë‹¹í•˜ëŠ” ë³¼ë¥¨ ê°’
            'loud': float('inf')
        }
        
        # ì ì‘í˜• ì„ê³„ê°’ ì¡°ì • (ê° ë ˆë²¨ì´ ìµœì†Œ 10% ì´ìƒ ë˜ë„ë¡)
        # ì´ìœ : ë°ì´í„°ê°€ í•œìª½ìœ¼ë¡œ ì¹˜ìš°ì³ìˆê±°ë‚˜ ë™ì¼ê°’ì´ ë§ì„ ë•Œ ë³€ë³„ë ¥ í™•ë³´
        self._adjust_volume_thresholds(volumes, initial_thresholds)
        
        print("\n=== ë³¼ë¥¨ ë ˆë²¨ ë¶„í¬ (ì ì‘í˜• ì„ê³„ê°’) ===")
        for level in ['soft', 'normal', 'loud']:
            count = np.sum(self._classify_volume(volumes) == level)
            percentage = (count / len(volumes)) * 100
            threshold_val = self.volume_stats['thresholds'][level]
            threshold_str = f"{threshold_val:.3f}" if threshold_val != float('inf') else "âˆ"
            print(f"{level}: {percentage:.1f}% (ì„ê³„ê°’: {threshold_str})")
        print(f"P30ê°’: {p30:.3f}, P70ê°’: {p70:.3f} (ë°±ë¶„ìœ„ ê¸°ì¤€ì )")

    def _adjust_volume_thresholds(self, volumes, initial_thresholds):
        """ì ì‘í˜• ì„ê³„ê°’ ì¡°ì • - ì •í™•ë„ í–¥ìƒì„ ìœ„í•œ í•µì‹¬ ë¡œì§"""
        def calculate_distribution(thresholds):
            levels = self._classify_volume(volumes, thresholds)
            unique, counts = np.unique(levels, return_counts=True)
            dist = dict(zip(unique, counts / len(volumes)))
            return {level: dist.get(level, 0.0) for level in ['soft', 'normal', 'loud']}

        # ì´ˆê¸° ì„ê³„ê°’ìœ¼ë¡œ ì‹œì‘ (P30, P70 ê°’ë“¤)
        current_thresholds = initial_thresholds.copy()
        distribution = calculate_distribution(current_thresholds)

        # ë°˜ë³µì  ì¡°ì •: ê° ë ˆë²¨ì´ ìµœì†Œ 10% ì´ìƒ í™•ë³´ë˜ë„ë¡
        # ëª©ì : ê·¹ë‹¨ ë¶„í¬/ë™ì¼ê°’ ì§‘ì¤‘ ë“±ì—ì„œë„ ë³€ë³„ë ¥ ìœ ì§€
        MAX_ITERATIONS = 20
        for iteration in range(MAX_ITERATIONS):
            if all(v >= 0.1 for v in distribution.values()):
                break  # ëª¨ë“  ë ˆë²¨ì´ 10% ì´ìƒ í™•ë³´ë¨

            # soft ë ˆë²¨ ì¡°ì •
            if distribution['soft'] < 0.1:  # softê°€ 10% ë¯¸ë§Œ
                current_thresholds['soft'] *= 1.1  # ì„ê³„ê°’ ìƒí–¥ â†’ ë” ë§ì€ êµ¬ê°„ì´ soft
            elif distribution['soft'] > 0.4:  # softê°€ 40% ì´ˆê³¼  
                current_thresholds['soft'] *= 0.9  # ì„ê³„ê°’ í•˜í–¥ â†’ soft êµ¬ê°„ ì¶•ì†Œ

            # loud ë ˆë²¨ ì¡°ì • (normal ì„ê³„ê°’ ì¡°ì •ìœ¼ë¡œ ì œì–´)
            if distribution['loud'] < 0.1:  # loudê°€ 10% ë¯¸ë§Œ
                current_thresholds['normal'] *= 0.9  # normal ì„ê³„ê°’ í•˜í–¥ â†’ loud êµ¬ê°„ í™•ëŒ€
            elif distribution['loud'] > 0.4:  # loudê°€ 40% ì´ˆê³¼
                current_thresholds['normal'] *= 1.1  # normal ì„ê³„ê°’ ìƒí–¥ â†’ loud êµ¬ê°„ ì¶•ì†Œ

            distribution = calculate_distribution(current_thresholds)

        self.volume_stats['thresholds'] = current_thresholds

    def _classify_volume(self, volumes, thresholds=None):
        """ë³¼ë¥¨ê°’ì„ 3ë‹¨ê³„ë¡œ ë¶„ë¥˜"""
        if thresholds is None:
            thresholds = self.volume_stats['thresholds']
            
        if thresholds['soft'] is None:
            return np.full(len(volumes), 'normal')
            
        levels = np.full(len(volumes), 'normal', dtype='U10')
        levels[volumes < thresholds['soft']] = 'soft'
        levels[volumes >= thresholds['normal']] = 'loud'
        return levels

    

    def assign_pitch_level(self, pitch):
        """í”¼ì¹˜ê°’ì— ë”°ë¼ level í• ë‹¹ (ì˜ë¯¸ìˆëŠ” ê·¹ë‹¨ê°’ ë³´ì¡´)"""
        if not self.pitch_stats['p10']:
            return 'normal'
        
        # 1ë‹¨ê³„: ì¸ê°„ ì²­ê° ê¸°ì¤€ ì ˆëŒ€ê°’ ì²´í¬ (ê·¹ë‹¨ê°’ ë³´ì¡´)
        if pitch < 80:  # ë§¤ìš° ë‚®ì€ ë‚¨ì„± ìŒì„±
            return 'low'
        elif pitch > 400:  # ë§¤ìš° ë†’ì€ ì—¬ì„± ìŒì„± ë˜ëŠ” ê°ì •ì  ë°œì„±
            return 'high'
        
        # 2ë‹¨ê³„: ìƒëŒ€ì  ë°±ë¶„ìœ„ ë¶„ë¥˜ (ì¼ë°˜ì ì¸ ê²½ìš°)
        if pitch <= self.pitch_stats['p10']:
            return 'low'
        elif pitch >= self.pitch_stats['p90']:
            return 'high'
        return 'normal'

    def analyze_speech_rate_distribution(self, segments):
        """ì „ì²´ ì„¸ê·¸ë¨¼íŠ¸ì˜ ë°œí™” ì†ë„ ë¶„í¬ ë¶„ì„"""
        rates = []
        
        # 1. ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ì˜ ë°œí™” ì†ë„ ìˆ˜ì§‘
        for segment in segments:
            duration = segment['end'] - segment['start']
            word_count = len(segment.get('words', []))
            
            if duration > 0 and word_count > 0:
                # ì´ˆë‹¹ ë‹¨ì–´ ìˆ˜ ê³„ì‚°
                rate = word_count / duration
                rates.append(rate)
                segment['_speech_rate'] = rate
        
        if rates:
            # 2. í†µê³„ê°’ ê³„ì‚°
            self.speech_rate_stats['values'] = rates
            self.speech_rate_stats['mean'] = np.mean(rates)
            self.speech_rate_stats['std'] = np.std(rates)
            
            # 3. 25%, 75% ë°±ë¶„ìœ„ìˆ˜ë¡œ ì„ê³„ê°’ ì„¤ì •
            p25 = np.percentile(rates, 25)
            p75 = np.percentile(rates, 75)
            
            self.speech_rate_stats['thresholds'] = {
                'slow': p25,
                'normal': p75,
                'fast': float('inf')
            }

    def assign_speech_rate_level(self, rate):
        """ë°œí™” ì†ë„ì— ë”°ë¥¸ level í• ë‹¹"""
        if not self.speech_rate_stats['thresholds']['slow']:
            return 'normal'
        
        if rate <= self.speech_rate_stats['thresholds']['slow']:
            return 'slow'
        elif rate >= self.speech_rate_stats['thresholds']['normal']:
            return 'fast'
        return 'normal'

    def _get_context_audio(self, audio, segment, pad_duration=0.1):
        """ì§§ì€ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ì˜¤ë””ì˜¤ íšë“"""
        start = max(0, segment['start'] - pad_duration)
        end = min(len(audio) / self.sample_rate, segment['end'] + pad_duration)
        
        start_idx = int(start * self.sample_rate)
        end_idx = int(end * self.sample_rate)
        
        return audio[start_idx:end_idx]

    def _calculate_phonetic_speech_rate(self, audio, word_text, duration):
        """ìŒì„±í•™ì ìœ¼ë¡œ ê°œì„ ëœ ë°œí™”ì†ë„ ê³„ì‚°"""
        if duration <= 0:
            return 1.0
            
        # 1. ìŒì„± ì—ë„ˆì§€ ê¸°ë°˜ ì‹¤ì œ ë°œì„± ì‹œê°„ ê³„ì‚°
        actual_speech_time = self._estimate_actual_speech_time(audio, duration)
        
        # 2. ìŒì ˆ ë³µì¡ë„ ê³ ë ¤
        syllable_complexity = self._estimate_syllable_complexity(word_text)
        
        # 3. ì¡°ì •ëœ ë°œí™”ì†ë„ = (ìŒì ˆ ë³µì¡ë„) / (ì‹¤ì œ ë°œì„± ì‹œê°„)
        if actual_speech_time > 0:
            adjusted_rate = syllable_complexity / actual_speech_time
        else:
            adjusted_rate = syllable_complexity / duration  # fallback
            
        return float(adjusted_rate)
    
    def _estimate_actual_speech_time(self, audio, total_duration):
        """ë¬´ìŒ êµ¬ê°„ì„ ì œì™¸í•œ ì‹¤ì œ ë°œì„± ì‹œê°„ ì¶”ì •"""
        if len(audio) == 0:
            return total_duration

        # ì—ë„ˆì§€ ê¸°ë°˜ ìŒì„± í™œë™ ê°ì§€ (ë²¡í„°í™”)
        frame_size = int(self.sample_rate * 0.025)  # 25ms í”„ë ˆì„
        hop_size = int(self.sample_rate * 0.010)    # 10ms í™‰

        # í”„ë ˆì„ RMS -> ì—ë„ˆì§€(=RMS^2)
        rms2d = librosa.feature.rms(
            y=audio.astype(np.float32),
            frame_length=frame_size,
            hop_length=hop_size,
            center=False
        )
        rms = rms2d.flatten()
        frame_energy = (rms ** 2).astype(np.float64)

        # ì ì‘ì  ì„ê³„ê°’ (ì „ì²´ ì˜¤ë””ì˜¤ ì—ë„ˆì§€ì˜ 5%)
        global_energy = float(np.mean(audio.astype(np.float32) ** 2))
        threshold = global_energy * 0.05

        if frame_energy.size == 0:
            return total_duration

        speech_frames = int(np.sum(frame_energy > threshold))
        total_frames = int(frame_energy.size)

        if total_frames > 0:
            speech_ratio = speech_frames / total_frames
            return total_duration * speech_ratio
        return total_duration
    
    def _estimate_syllable_complexity(self, text):
        """í…ìŠ¤íŠ¸ì˜ ìŒì ˆ ë³µì¡ë„ ì¶”ì •"""
        if not text:
            return 1.0
            
        # ê¸°ë³¸ ê¸€ì ìˆ˜
        char_count = len(text.strip())
        
        # ì–¸ì–´ë³„ ê°€ì¤‘ì¹˜ ì ìš©
        complexity = 0
        for char in text:
            if char.isspace():
                continue
            elif 0x1100 <= ord(char) <= 0x11FF or 0x3130 <= ord(char) <= 0x318F or 0xAC00 <= ord(char) <= 0xD7AF:
                # í•œê¸€: ììŒ+ëª¨ìŒ êµ¬ì¡°ë¡œ ë³µì¡
                complexity += 1.2
            elif char.isalpha():
                # ì˜ì–´: ìƒëŒ€ì ìœ¼ë¡œ ë‹¨ìˆœ
                complexity += 1.0
            else:
                # ìˆ«ì, ê¸°í˜¸
                complexity += 0.8
                
        return max(complexity, 1.0)