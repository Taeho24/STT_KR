import torch
import numpy as np
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    AutoFeatureExtractor,
    AutoModelForAudioClassification
)
import librosa
import logging
import json
import os
from pathlib import Path
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import torch.nn.functional as F
from tqdm import tqdm

from .config import config
from .db_manager import DBManager

DEFAULT_AUDIO_MODEL = "xbgoose/hubert-large-speech-emotion-recognition-russian-dusha-finetuned"
DEFAULT_TEXT_MODEL = "j-hartmann/emotion-english-distilroberta-base"

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler('emotion_classification.log'),
        logging.StreamHandler()
    ]
)

@dataclass
class EmotionResult:
    emotion: str
    confidence: float
    features: Dict[str, float]
    text_score: float
    audio_score: float
    text_distribution: Dict[str, float]
    audio_distribution: Dict[str, float]
    combined_distribution: Dict[str, float]

class EmotionClassifier:
    """ê°ì • ë¶„ë¥˜ ëª¨ë¸ê³¼ ì•™ìƒë¸” ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(
        self,
        device: str = "cuda" if torch.cuda.is_available() else "cpu",
        batch_size: int = 8,
        cache_dir: str = ".cache",
        audio_model_name: str | None = None,
        text_model_name: str | None = None,
        enable_text: bool | None = None, 
        task_id: str | None = None,
    ):
        self.device = device
        self.batch_size = batch_size
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.db_manager = DBManager(task_id=task_id)

        # ëª¨ë¸ ì´ë¦„ êµ¬ì„± (config ìš°ì„ , íŒŒë¼ë¯¸í„°ë¡œ ì¬ì •ì˜ ê°€ëŠ¥)
        configured_audio = config.get('models', 'audio', DEFAULT_AUDIO_MODEL)
        configured_text = config.get('models', 'text', DEFAULT_TEXT_MODEL)

        self.audio_model_name = audio_model_name or configured_audio or DEFAULT_AUDIO_MODEL
        resolved_text_name: Optional[str]
        if text_model_name is None:
            resolved_text_name = configured_text or DEFAULT_TEXT_MODEL
        else:
            resolved_text_name = text_model_name or None

        self.available_audio_models = config.get('models', 'audio_candidates', [])
        self.available_text_models = config.get('models', 'text_candidates', [])
        
        # === ìµœì í™”ëœ ë‹¤ì–¸ì–´ ê°ì • ë¶„ì„ ëª¨ë¸ ===
        # XLS-R ê¸°ë°˜ - í•œêµ­ì–´/ì˜ì–´ ì§€ì›, ë¹ ë¥¸ ì¶”ë¡ 
        print(f"ğŸ”„ Loading audio emotion model: {self.audio_model_name}")
        
        try:
            # 1ì°¨ ì‹œë„: XLS-R ê¸°ë°˜ ê°ì • ì¸ì‹ ëª¨ë¸ (ë” ë¹ ë¥´ê³  íš¨ìœ¨ì )
            # FP32 ì‚¬ìš© - ì •í™•ë„ ìš°ì„  (FP16ì€ ê°ì • ë¶„ë¥˜ì—ì„œ ì •ë°€ë„ ì†ì‹¤ í¼)
            self.audio_model = AutoModelForAudioClassification.from_pretrained(
                self.audio_model_name,
                cache_dir=str(self.cache_dir),
                torch_dtype=torch.float32  # ì •í™•ë„ ìš°ì„ : FP32 ê³ ì •
            ).to(device)
            
            self.feature_extractor = AutoFeatureExtractor.from_pretrained(
                self.audio_model_name,
                cache_dir=str(self.cache_dir)
            )
            self.feature_sampling_rate = getattr(self.feature_extractor, "sampling_rate", 16000)
            print("âœ… Audio emotion model loaded successfully (FP32 for accuracy)")
            
        except Exception as e:
            logging.error(f"Failed to load audio model: {str(e)}")
            raise

        # ëª¨ë¸ë³„ ë³´ì •(ìº˜ë¦¬ë¸Œë ˆì´ì…˜) ì œê±°: ë‹¨ìˆœí™”(í˜„ì¬ ê¸°ë³¸ xbgooseì—ëŠ” ë¶ˆí•„ìš”)
        self._calibration: Dict[str, float] = {}
        
        # í…ìŠ¤íŠ¸ ëª¨ë¸ì€ ë³´ì¡° ìˆ˜ë‹¨ìœ¼ë¡œë§Œ ì‚¬ìš© (ë‹¤ì–¸ì–´ ì§€ì›)
        self.weights = config.get('emotions', 'weights') or {}
        self.audio_weight = float(self.weights.get('audio', 1.0))
        self.text_weight = float(self.weights.get('text', 0.0))

        if enable_text is None:
            text_enabled = self.text_weight > 0 and resolved_text_name is not None
        else:
            text_enabled = enable_text and resolved_text_name is not None

        self.text_enabled = text_enabled
        if self.text_enabled:
            self.text_model_name = resolved_text_name
            print(f"ğŸ”„ Loading text emotion model: {self.text_model_name}")
            self.text_model = AutoModelForSequenceClassification.from_pretrained(
                self.text_model_name,  # ì¼ë‹¨ ìœ ì§€ (í…ìŠ¤íŠ¸ëŠ” ë³´ì¡°)
                cache_dir=str(self.cache_dir),
                torch_dtype=torch.float32  # ì •í™•ë„ ìš°ì„ : FP32 ê³ ì •
            ).to(device)
            
            self.text_tokenizer = AutoTokenizer.from_pretrained(
                self.text_model_name,
                cache_dir=str(self.cache_dir)
            )
        else:
            self.text_model_name = None
            self.text_model = None
            self.text_tokenizer = None
            self.text_weight = 0.0

        # configì—ì„œ ëª¨ë“  ì„¤ì • ë¡œë“œ
        self.emotion_mapping = config.get('emotions', 'mapping')
        self.emotion_weights = config.get('emotions', 'emotion_weights')
        self.excluded_emotions = config.get('emotions', 'exclude', default=[]) or []
        self.audio_temperature = config.get('emotions', 'audio_temperature', default=1.0) or 1.0
        self.ensemble_settings = config.get('emotions', 'ensemble', default={}) or {}
        self.emotion_colors = self.db_manager.load_config()['hex_colors']['emotion_colors']
        self.default_color = self.db_manager.load_config()['hex_colors']['default_color']

        self.audio_confidence_threshold = float(self.ensemble_settings.get('audio_confidence_threshold', 0.6))
        self.text_confidence_threshold = float(self.ensemble_settings.get('text_confidence_threshold', 0.55))
        self.dominance_margin = float(self.ensemble_settings.get('dominance_margin', 0.15))
        self.audio_confidence_boost = float(self.ensemble_settings.get('audio_confidence_boost', 1.3))
        self.audio_confidence_decay = float(self.ensemble_settings.get('audio_confidence_decay', 0.7))
        self.text_confidence_boost = float(self.ensemble_settings.get('text_confidence_boost', 1.15))
        self.text_confidence_decay = float(self.ensemble_settings.get('text_confidence_decay', 0.8))
        self.neutral_suppression = float(self.ensemble_settings.get('neutral_suppression', 0.7))
        self.neutral_floor = float(self.ensemble_settings.get('neutral_floor', 0.05))
        # ì¤‘ë¦½ ì„ í˜¸ ê°€ë“œ ë° ì‹œê°„ ìŠ¤ë¬´ë”© ì„¤ì •
        self.neutral_guard = config.get('emotions', 'neutral_guard', default={}) or {}
        self.ng_enabled = bool(self.neutral_guard.get('enabled', True))
        self.ng_min_audio_conf = float(self.neutral_guard.get('min_audio_conf', 0.62))
        self.ng_min_audio_margin = float(self.neutral_guard.get('min_audio_margin', 0.18))
        self.ng_min_text_support = float(self.neutral_guard.get('min_text_support', 0.22))
        self.ng_damp_factor = float(self.neutral_guard.get('damp_factor', 0.35))
        self.ng_targets = set(self.neutral_guard.get('target_classes', ['happy', 'angry']))
        # AV ìœµí•© ì„¤ì •(ê²°í• í´ë˜ìŠ¤ ë³´ê°•ìš©)
        self.av_fusion = config.get('emotions', 'av_fusion', default={}) or {}
        self.av_enabled = bool(self.av_fusion.get('enabled', True))
        self.av_text_gain = float(self.av_fusion.get('text_gain', 0.4))
        self.av_audio_gain = float(self.av_fusion.get('audio_gain', 0.15))
        
        # Whisper ëª¨ë¸ ë ˆì´ë¸” ë§¤í•‘ (ë‹¤ì–‘í•œ í‘œí˜„ì„ 7ê°œ ê°ì •ìœ¼ë¡œ í†µí•©)
        self.whisper_emotion_mapping = {
            'happy': 'happy',
            'happiness': 'happy',
            'joy': 'happy',
            'excited': 'happy',
            'sad': 'sad',
            'sadness': 'sad',
            'angry': 'angry',
            'anger': 'angry',
            'fear': 'fear',
            'fearful': 'fear',
            'surprise': 'surprise',
            'surprised': 'surprise',
            'disgust': 'disgust',
            'disgusted': 'disgust',
            'neutral': 'neutral',
            'calm': 'neutral',
            'bored': 'neutral'
        }

        self._setup_memory_management()
        logging.info("Emotion classifier initialized successfully")

        # í…ìŠ¤íŠ¸/ì˜¤ë””ì˜¤ ëª¨ë¸ ë ˆì´ë¸” ì¶œë ¥
        if self.text_enabled and self.text_model is not None:
            print("Text model labels:", self.text_model.config.id2label)
        print("Audio model labels:", self.audio_model.config.id2label)

    def _apply_emotion_exclusions(self, scores: Dict[str, float]) -> Dict[str, float]:
        """ì œì™¸ ì„¤ì •ëœ ê°ì • ë ˆì´ë¸”ì„ 0ìœ¼ë¡œ ë§Œë“¤ê³  ì¬ì •ê·œí™”.

        - ëª¨ë“  ì ìˆ˜ê°€ 0ì´ ë˜ë©´ ì›ë³¸ ìœ ì§€(ì•ˆì •ì„±), ë˜ëŠ” neutral=1.0ë¡œ ëŒ€ì²´í•  ìˆ˜ ìˆìŒ.
        """
        if not self.excluded_emotions:
            return scores
        if not isinstance(scores, dict):
            return scores
        out = dict(scores)
        changed = False
        for ex in self.excluded_emotions:
            if ex in out:
                out[ex] = 0.0
                changed = True
        if not changed:
            return out
        s = sum(out.values())
        if s > 1e-12:
            return {k: (v / s) for k, v in out.items()}
        # ëª¨ë“  ì ìˆ˜ê°€ 0ì´ë©´ neutral=1.0ë¡œ ë³µêµ¬(ë³´ìˆ˜ì )
        out = {k: 0.0 for k in out.keys()}
        out['neutral'] = 1.0
        return out

    def _av_compatibility(self, segment: Dict[str, Any]) -> Dict[str, float]:
        """AV(Valence/Arousal) ê·¼ì‚¬ê°’ì„ ë°”íƒ•ìœ¼ë¡œ ê²°í• í´ë˜ìŠ¤(disgust/fear/surprise) í˜¸í™˜ì„±ì„ ê³„ì‚°.

        - arousal, valence âˆˆ [0,1] ê°€ì •(ì—†ìœ¼ë©´ 0.5ë¡œ ëŒ€ì²´)
        - í…œí”Œë¦¿ì€ 'ë„“ì€' ê°€ìš°ì‹œì•ˆ í˜•íƒœë¡œ ìœ ì—°ì„±ì„ í™•ë³´
          Â· surprise: arousalâ†‘(Î¼=0.75, Ïƒâ‰ˆ0.25), valence ì¤‘ë¦½(Î¼=0.5, Ïƒâ‰ˆ0.35)
          Â· fear:    arousalâ†‘(Î¼=0.75, Ïƒâ‰ˆ0.25), valence ìŒ(Î¼=0.25, Ïƒâ‰ˆ0.25)
          Â· disgust: valence ìŒ(Î¼=0.25, Ïƒâ‰ˆ0.28), arousal ì–‘ë´‰í˜•(ì €/ê³  ëª¨ë‘ í—ˆìš©: Î¼â‰ˆ0.3, 0.7, Ïƒâ‰ˆ0.22)
        """
        try:
            av = segment.get('av') or segment.get('voice_analysis', {}).get('av')
            if not isinstance(av, dict):
                raise ValueError("No AV in segment")
            a = float(av.get('arousal', 0.5))
            v = float(av.get('valence', 0.5))
        except Exception:
            a, v = 0.5, 0.5

        def gauss(x: float, mu: float, sigma: float) -> float:
            sigma = max(1e-3, sigma)
            return float(np.exp(-((x - mu) / sigma) ** 2))

        # Surprise: ê³ ê°ì„± + ì¤‘ë¦½ ë°¸ëŸ°ìŠ¤
        s_ar = gauss(a, 0.75, 0.25)
        s_va = gauss(v, 0.50, 0.35)
        comp_surprise = 0.5 * (s_ar + s_va)

        # Fear: ê³ ê°ì„± + ë¶€ì • ë°¸ëŸ°ìŠ¤
        f_ar = gauss(a, 0.75, 0.25)
        f_va = gauss(v, 0.25, 0.25)
        comp_fear = 0.5 * (f_ar + f_va)

        # Disgust: ë¶€ì • ë°¸ëŸ°ìŠ¤ + (ì €/ê³ ) ì–‘ë´‰í˜• ê°ì„± í—ˆìš©
        d_va = gauss(v, 0.25, 0.28)
        d_ar_low = gauss(a, 0.30, 0.22)
        d_ar_high = gauss(a, 0.70, 0.22)
        d_ar = max(d_ar_low, d_ar_high)  # ë‘˜ ì¤‘ ë” ì˜ ë§ëŠ” ìª½ì„ ì±„íƒ
        comp_disgust = 0.5 * (d_ar + d_va)

        # ì•ˆì •ì„± ì°¨ì›ì—ì„œ [0,1] í´ë¨í”„
        def clamp01(x: float) -> float:
            return float(np.clip(x, 0.0, 1.0))

        return {
            'surprise': clamp01(comp_surprise),
            'fear': clamp01(comp_fear),
            'disgust': clamp01(comp_disgust)
        }

    def _setup_memory_management(self):
        """ë©”ëª¨ë¦¬ ê´€ë¦¬ ì„¤ì •"""
        if torch.cuda.is_available():
            # GPU ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ í•¨ìˆ˜
            torch.cuda.empty_cache()
            # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
            torch.set_grad_enabled(False)
        
        # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì„ê³„ê°’ ì„¤ì •
        self.max_audio_length = 30  # ìµœëŒ€ 30ì´ˆ
        self.max_text_length = 512  # BERT ëª¨ë¸ ì œí•œ
        self.audio_sampling_rate = 16000

    def process_batch(self, segments: List[Dict[str, Any]], audio_data: np.ndarray, sr: int = 16000):
        """ë°°ì¹˜ ë‹¨ìœ„ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ """
        results = []
        
        # ì§„í–‰ë¥  í‘œì‹œì™€ í•¨ê»˜ ë°°ì¹˜ ì²˜ë¦¬
        for i in tqdm(range(0, len(segments), self.batch_size), desc="Processing segments"):
            batch = segments[i:i + self.batch_size]
            
            # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
            batch_audio = []
            for seg in batch:
                try:
                    start_idx = int(seg['start'] * sr)
                    end_idx = int(seg['end'] * sr)
                    if start_idx < len(audio_data) and end_idx <= len(audio_data) and start_idx < end_idx:
                        audio_segment = audio_data[start_idx:end_idx]
                        batch_audio.append(audio_segment)
                    else:
                        # ì¸ë±ìŠ¤ ë²”ìœ„ ì´ˆê³¼ ì‹œ ëŒ€ì²´ ë°ì´í„°
                        batch_audio.append(np.zeros(1600, dtype=np.float32))
                except Exception as e:
                    logging.warning(f"Audio segment extraction error: {str(e)}")
                    batch_audio.append(np.zeros(1600, dtype=np.float32))
            batch_text = [seg.get('text', '') for seg in batch]

            # ë°°ì¹˜ ì²˜ë¦¬
            batch_results = self._process_segment_batch(batch, batch_audio, batch_text)
            results.extend(batch_results)

            # ë©”ëª¨ë¦¬ ê´€ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        return results

    def _process_segment_batch(self, segments, batch_audio, batch_text):
        """ë°°ì¹˜ ë‹¨ìœ„ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬"""
        results = []
        
        # í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„
        text_scores = self._analyze_text_batch(batch_text)
        
        # ì˜¤ë””ì˜¤ ê°ì • ë¶„ì„
        audio_scores = self._analyze_audio_batch(batch_audio)
        
        # ê²°ê³¼ ê²°í•©
        for i, segment in enumerate(segments):
            text_score = text_scores[i]
            audio_score = audio_scores[i]
            
            # ìµœì¢… ê°ì • ê²°ì •
            final_emotion = self._combine_predictions(
                text_score,
                audio_score,
                segment
            )
            
            results.append(final_emotion)
            
            # ë¡œê·¸ ê¸°ë¡
            self._log_segment_result(segment, final_emotion)
            
        return results

    def _analyze_text_batch(self, texts: List[str]) -> List[Dict[str, float]]:
        """í…ìŠ¤íŠ¸ ë°°ì¹˜ ê°ì • ë¶„ì„"""
        if not texts:
            return [{"neutral": 1.0}] * len(texts)

        if not self.text_enabled or self.text_model is None or self.text_tokenizer is None:
            return [{} for _ in texts]

        try:
            inputs = self.text_tokenizer(
                texts,
                padding=True,
                truncation=True,
                max_length=self.max_text_length,
                return_tensors="pt"
            ).to(self.device)

            with torch.no_grad():
                # ì •í™•ë„ ìš°ì„ : FP32 ì¶”ë¡  (FP16 autocast ì œê±°)
                outputs = self.text_model(**inputs)
                scores = F.softmax(outputs.logits, dim=-1)

            results = []
            for score in scores:
                # ê°ì • í‘œì¤€í™” ì¶”ê°€
                emotion_scores = {}
                for i, emotion in enumerate(self.text_model.config.id2label.values()):
                    # ê°ì • ë ˆì´ë¸” ì†Œë¬¸ì ë³€í™˜ ë° ë§¤í•‘
                    standardized_emotion = emotion.lower()
                    if standardized_emotion in ['sadness', 'sad']:
                        standardized_emotion = 'sad'
                    elif standardized_emotion in ['anger', 'angry']:
                        standardized_emotion = 'angry'
                    elif standardized_emotion in ['joy', 'happy', 'positive']:  # joy/positiveë¥¼ happyë¡œ ë§¤í•‘
                        standardized_emotion = 'happy'
                    elif standardized_emotion in ['other']:
                        standardized_emotion = 'neutral'
                    emotion_scores[standardized_emotion] = score[i].item()
                # ì œì™¸ ë ˆì´ë¸” ì ìš© í›„ ì •ê·œí™”
                cleaned = self._apply_emotion_exclusions(emotion_scores)
                results.append(cleaned)

            return results
        except Exception as e:
            logging.error(f"Text analysis error: {str(e)}")
            return [{"neutral": 1.0}] * len(texts)

    def _analyze_audio_batch(self, audio_segments: List[np.ndarray]) -> List[Dict[str, float]]:
        """ì˜¤ë””ì˜¤ ë°°ì¹˜ ê°ì • ë¶„ì„"""
        if not audio_segments:
            return [{"neutral": 1.0}] * len(audio_segments)

        try:
            feature_sr = getattr(self, "feature_sampling_rate", 16000)
            base_sr = getattr(self, "audio_sampling_rate", feature_sr)
            max_length_samples = int(feature_sr * self.max_audio_length)
            min_length_samples = max(1, int(feature_sr * 0.1))
            model_type = getattr(self.audio_model.config, "model_type", "").lower()
            requires_fixed_length = model_type == "whisper"

            valid_segments = []
            for segment in audio_segments:
                if not isinstance(segment, np.ndarray) or segment.size == 0:
                    segment = np.zeros(min_length_samples, dtype=np.float32)
                else:
                    if segment.ndim > 1:
                        segment = np.squeeze(segment)
                    segment = np.asarray(segment, dtype=np.float32)

                if base_sr != feature_sr:
                    segment = librosa.resample(segment, orig_sr=base_sr, target_sr=feature_sr)

                if requires_fixed_length:
                    if len(segment) > max_length_samples:
                        segment = segment[:max_length_samples]
                    elif len(segment) < max_length_samples:
                        segment = np.pad(segment, (0, max_length_samples - len(segment)))
                else:
                    if len(segment) < min_length_samples:
                        segment = np.pad(segment, (0, min_length_samples - len(segment)))
                    if len(segment) > max_length_samples:
                        segment = segment[:max_length_samples]

                valid_segments.append(segment)

            if not valid_segments:
                return [{"neutral": 1.0}] * len(audio_segments)

            feature_kwargs = {
                "sampling_rate": feature_sr,
                "return_tensors": "pt",
            }
            if requires_fixed_length:
                feature_kwargs.update({
                    "padding": "max_length",
                    "max_length": max_length_samples,
                    "truncation": True,
                    "return_attention_mask": False,
                })
            else:
                feature_kwargs.update({
                    "padding": True,
                    "return_attention_mask": True,
                })

            features = self.feature_extractor(valid_segments, **feature_kwargs)

            if requires_fixed_length and isinstance(features, dict) and "attention_mask" in features:
                features.pop("attention_mask")

            features = features.to(self.device)

            with torch.no_grad():
                # ì •í™•ë„ ìš°ì„ : FP32 ì¶”ë¡  (FP16 autocast ì œê±°)
                outputs = self.audio_model(**features)
                logits = outputs.logits
                temperature = max(float(self.audio_temperature), 1e-3)
                if temperature != 1.0:
                    logits = logits / temperature

                scores = F.softmax(logits, dim=-1)

                # ê³¼ë„í•œ í‰ì¤€í™”ë¥¼ ë°©ì§€í•˜ë©´ì„œë„ 0 í™•ë¥ ì„ í”¼í•˜ê¸° ìœ„í•´ ì•„ì£¼ ì‘ì€ í•˜í•œë§Œ ì ìš©
                scores = torch.clamp(scores, min=1e-6)
                scores = scores / scores.sum(dim=-1, keepdim=True)

            results = []
            emotion_aliases = {
                'happiness': 'happy',
                'joy': 'happy',
                'excited': 'happy',
                'positive': 'happy',
                'hap': 'happy',
                'ê¸°ì¨': 'happy',
                'í–‰ë³µ': 'happy',
                'anger': 'angry',
                'ang': 'angry',
                'ë¶„ë…¸': 'angry',
                'í™”ë‚¨': 'angry',
                'sadness': 'sad',
                'sad': 'sad',
                'ìŠ¬í””': 'sad',
                'fearful': 'fear',
                'fear': 'fear',
                'ë¶ˆì•ˆ': 'fear',
                'ê³µí¬': 'fear',
                'surprised': 'surprise',
                'surprise': 'surprise',
                'ë‹¹í™©': 'surprise',
                'ë†€ëŒ': 'surprise',
                'disgusted': 'disgust',
                'disgust': 'disgust',
                'ì§œì¦': 'disgust',
                'neutral': 'neutral',
                'other': 'neutral',
                'neu': 'neutral',
                'ì¤‘ë¦½': 'neutral',
                'calm': 'neutral',
                'bored': 'neutral'
            }

            for score in scores:
                # ì›ë³¸ ë ˆì´ë¸”ì„ 7ê°œ ê°ì •ìœ¼ë¡œ ë§¤í•‘
                raw_emotion_scores = {
                    emotion: score[i].item()
                    for i, emotion in enumerate(self.audio_model.config.id2label.values())
                }
                
                # í‘œì¤€ 7ê°œ ê°ì •ìœ¼ë¡œ í†µí•© (ê¸°ì¡´ ë§¤í•‘ ì‚¬ìš©)
                emotion_scores = {
                    'happy': 0.0, 'sad': 0.0, 'angry': 0.0, 'fear': 0.0,
                    'surprise': 0.0, 'disgust': 0.0, 'neutral': 0.0
                }
                
                for raw_emotion, raw_score in raw_emotion_scores.items():
                    normalized = raw_emotion.strip().lower()
                    mapped = emotion_aliases.get(raw_emotion, None)
                    if mapped is None:
                        mapped = emotion_aliases.get(normalized, None)
                    if mapped is None:
                        mapped = normalized

                    if mapped in emotion_scores:
                        emotion_scores[mapped] += raw_score
                    else:
                        emotion_scores['neutral'] += raw_score
                
                # === ëª¨ë¸ë³„ í›„ì²˜ë¦¬ ë³´ì • ì ìš© (ì˜ˆ: ehcalabres ì¤‘ë¦½ ì ë¦¼ ì™„í™”) ===
                if self._calibration:
                    ns = float(self._calibration.get("neutral_scale", 1.0))
                    nns = float(self._calibration.get("non_neutral_scale", 1.0))
                    if ns != 1.0 or nns != 1.0:
                        # ì¤‘ë¦½ì€ ns ë°°, ë¹„ì¤‘ë¦½ì€ nns ë°°ë¡œ ìŠ¤ì¼€ì¼ ì¡°ì •
                        for k in list(emotion_scores.keys()):
                            if k == 'neutral':
                                emotion_scores[k] *= ns
                            else:
                                emotion_scores[k] *= nns

                # ì •ê·œí™”
                total = sum(emotion_scores.values())
                if total > 0:
                    emotion_scores = {k: v/total for k, v in emotion_scores.items()}
                # ì œì™¸ ë ˆì´ë¸” ì ìš© í›„ ì •ê·œí™”
                cleaned = self._apply_emotion_exclusions(emotion_scores)
                results.append(cleaned)

            return results

        except Exception as e:
            logging.error(f"Audio analysis error: {str(e)}")
            return [{"neutral": 1.0}] * len(audio_segments)

    def _combine_predictions(
        self,
        text_scores: Dict[str, float],
        audio_scores: Dict[str, float],
        segment: Dict[str, Any]
    ) -> EmotionResult:
        """ë©€í‹°ëª¨ë‹¬ ì˜ˆì¸¡ ê²°ê³¼ ê²°í•©"""
        combined_scores = {}

        # ëª¨ë‹¬ë³„ ìƒìœ„ ê°ì • ë° ì‹ ë¢°ë„ ê³„ì‚°
        sorted_audio = sorted(audio_scores.items(), key=lambda x: x[1], reverse=True)
        sorted_text = sorted(text_scores.items(), key=lambda x: x[1], reverse=True)

        audio_top = sorted_audio[0] if sorted_audio else ("neutral", 0.0)
        audio_second = sorted_audio[1] if len(sorted_audio) > 1 else ("neutral", 0.0)
        text_top = sorted_text[0] if sorted_text else ("neutral", 0.0)

        audio_margin = audio_top[1] - audio_second[1]
        high_audio = audio_top[1] >= self.audio_confidence_threshold and audio_margin >= self.dominance_margin
        high_text = text_top[1] >= self.text_confidence_threshold

        # ë™ì  ê°€ì¤‘ì¹˜ ì¡°ì •
        adjusted_audio_weight = self.audio_weight
        adjusted_text_weight = self.text_weight

        if high_audio and audio_top[0] != "neutral":
            adjusted_audio_weight *= self.audio_confidence_boost
            adjusted_text_weight *= self.text_confidence_decay

        if (not high_audio or audio_top[0] == "neutral") and high_text and text_top[0] != "neutral":
            adjusted_text_weight *= self.text_confidence_boost
            adjusted_audio_weight *= self.audio_confidence_decay

        if audio_top[0] == text_top[0] and audio_top[0] != "neutral" and (high_audio or high_text):
            # ë‘ ëª¨ë‹¬ì´ í•©ì¹˜ë©´ ë™ì¼ ë¹„ìœ¨ë¡œ ì†Œí­ ê°•í™”
            adjusted_audio_weight *= 1.05
            adjusted_text_weight *= 1.05

        weight_sum = adjusted_audio_weight + adjusted_text_weight
        if weight_sum > 0:
            dynamic_audio_weight = adjusted_audio_weight / weight_sum
            dynamic_text_weight = adjusted_text_weight / weight_sum
        else:
            dynamic_audio_weight = self.audio_weight
            dynamic_text_weight = self.text_weight

        # í…ìŠ¤íŠ¸ì™€ ì˜¤ë””ì˜¤ ì ìˆ˜ ê²°í•©
        for emotion in set(text_scores.keys()) | set(audio_scores.keys()):
            text_score = text_scores.get(emotion, 0.0)
            audio_score = audio_scores.get(emotion, 0.0)
            
            # ê°ì •ë³„ ê°€ì¤‘ì¹˜ ì ìš©
            emotion_weight = self.emotion_weights.get(emotion, 1.0)
            
            combined_scores[emotion] = (
                (text_score * dynamic_text_weight +
                 audio_score * dynamic_audio_weight) * emotion_weight
            )

        # === AV ìœµí•©ìœ¼ë¡œ ê²°í• í´ë˜ìŠ¤(disgust/fear/surprise) ë³´ê°• ===
        if self.av_enabled:
            av_comp = self._av_compatibility(segment)
            for k in ('disgust', 'fear', 'surprise'):
                c = float(av_comp.get(k, 0.5))
                # í…ìŠ¤íŠ¸ ê°•í•œ ì£¼ì¥ì— í˜¸í™˜ì„± ë³´ë„ˆìŠ¤, ê²°í• í´ë˜ìŠ¤ì˜ ì˜ì‚¬-ì˜¤ë””ì˜¤ ì ìˆ˜ë„ ì†Œí­ ë¶€ì—¬
                combined_scores[k] = combined_scores.get(k, 0.0) \
                    + self.av_text_gain * c * float(text_scores.get(k, 0.0)) \
                    + self.av_audio_gain * c

        # === Disgust ê²Œì´íŒ…: í…ìŠ¤íŠ¸ ì‹ í˜¸ê°€ ì¶©ë¶„í•˜ê³  ì˜¤ë””ì˜¤ê°€ ë¶ˆí™•ì‹¤/ì¤‘ë¦½ì¼ ë•Œë§Œ í†µê³¼ ===
        if "disgust" in combined_scores:
            text_disgust = float(text_scores.get("disgust", 0.0)) if isinstance(text_scores, dict) else 0.0
            audio_neutral = float(audio_scores.get("neutral", 0.0)) if isinstance(audio_scores, dict) else 0.0
            # ì˜¤ë””ì˜¤ ë¶ˆí™•ì‹¤ì„±: 1ìœ„-2ìœ„ ê²©ì°¨ê°€ ì‘ì„ ë•Œ
            audio_uncertain = (audio_margin < max(0.12, 0.5 * self.dominance_margin))
            # í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ…(ê°„ë‹¨): ì—­ê²¨/ì§•ê·¸ëŸ¬/gross/disgust ë“± í¬í•¨ ì‹œ +0.05
            kw = ["ì—­ê²¨", "ì§•ê·¸ëŸ¬", "í˜ì˜¤", "gross", "yuck", "ew", "disgust"]
            text_raw = (segment.get("text", "") or "").lower()
            if any(k in text_raw for k in kw):
                text_disgust += 0.05
            # ê²Œì´íŠ¸ ì¡°ê±´ (AV í˜¸í™˜ì„±ì´ ë†’ì„ìˆ˜ë¡ ê²Œì´íŠ¸ ì™„í™”)
            av_bonus = 0.0
            if self.av_enabled:
                av_bonus = 0.08 * float(self._av_compatibility(segment).get('disgust', 0.5))
            disgust_gate = ((text_disgust + av_bonus) >= 0.65) and (audio_neutral >= 0.60 or audio_uncertain)
            if not disgust_gate:
                # í†µê³¼ ì‹¤íŒ¨: disgust ì–µì œ â†’ 0ìœ¼ë¡œ ë‘ê³  ì¬ë¶„ë°°ëŠ” ì •ê·œí™”ë¡œ ì²˜ë¦¬
                combined_scores["disgust"] = 0.0

        # ì¤‘ë¦½ ê°ì •ì´ ê³¼ë„í•˜ê²Œ ì§€ë°°í•˜ëŠ” í˜„ìƒì„ ì™„í™”
        if 'neutral' in combined_scores:
            suppress_neutral = False
            if high_audio and audio_top[0] != 'neutral':
                suppress_neutral = True
            if high_text and text_top[0] != 'neutral' and audio_top[0] != 'neutral':
                suppress_neutral = True

            if suppress_neutral:
                combined_scores['neutral'] *= self.neutral_suppression

            total_before_floor = sum(combined_scores.values())
            if total_before_floor > 0:
                min_neutral = self.neutral_floor * total_before_floor
                if combined_scores['neutral'] < min_neutral:
                    combined_scores['neutral'] = min_neutral

        total_combined = sum(combined_scores.values())
        if total_combined > 0:
            normalized_combined = {k: v / total_combined for k, v in combined_scores.items()}
        else:
            normalized_combined = combined_scores

        # ìµœì¢… ê°ì • ì„ íƒ
        # === Neutral guard: ë¶ˆí™•ì‹¤í•œ happy/angryëŠ” ì¤‘ë¦½ìœ¼ë¡œ ê¸°ìš¸ì„ ===
        if self.ng_enabled:
            # ëª¨ë‹¬ë³„ 1,2ìœ„ ì¬ì‚¬ìš©
            sorted_audio = sorted(audio_scores.items(), key=lambda x: x[1], reverse=True)
            audio_top = sorted_audio[0] if sorted_audio else ("neutral", 0.0)
            audio_second = sorted_audio[1] if len(sorted_audio) > 1 else ("neutral", 0.0)
            audio_margin = audio_top[1] - audio_second[1]
            sorted_text = sorted(text_scores.items(), key=lambda x: x[1], reverse=True) if isinstance(text_scores, dict) else []
            text_top = sorted_text[0] if sorted_text else ("neutral", 0.0)

            for cls in list(self.ng_targets):
                cur = normalized_combined.get(cls, 0.0)
                if cur <= 0:
                    continue
                text_support = float(text_scores.get(cls, 0.0)) if isinstance(text_scores, dict) else 0.0
                # í…ìŠ¤íŠ¸ê°€ ë°˜ëŒ€ ì •ì„œ(í–‰ë³µâ†”ë¶„ë…¸/ìŠ¬í””/ê³µí¬/í˜ì˜¤)ë¡œ ê°•í•˜ê²Œ ì£¼ì¥í•˜ë©´ ê°€ì¤‘ ê°ì‡ 
                neg_text = max(
                    float(text_scores.get('angry', 0.0)),
                    float(text_scores.get('sad', 0.0)),
                    float(text_scores.get('fear', 0.0)),
                    float(text_scores.get('disgust', 0.0))
                ) if isinstance(text_scores, dict) else 0.0
                conflict = (cls == 'happy' and neg_text >= 0.40) or (cls == 'angry' and float(text_scores.get('happy', 0.0)) >= 0.40)

                # ì˜¤ë””ì˜¤/í…ìŠ¤íŠ¸ê°€ í•´ë‹¹ clsë¥¼ ê°•í•˜ê²Œ ì£¼ì¥í•˜ëŠ”ì§€ ì—¬ë¶€
                audio_supports = (audio_top[0] == cls)
                text_supports = (text_top[0] == cls)
                audio_strong = audio_supports and (audio_top[1] >= self.ng_min_audio_conf) and (audio_margin >= self.ng_min_audio_margin)
                text_strong = text_supports and (text_top[1] >= max(0.35, self.text_confidence_threshold))
                bimodal_agree = audio_supports and text_supports

                # ê°€ë“œ ì¡°ê±´: ì˜¤ë””ì˜¤ê°€ ê°•í•˜ì§€ ì•Šê³  í…ìŠ¤íŠ¸ ì§€ì§€ê°€ ì•½í•˜ê±°ë‚˜, ì •ì„œ ì¶©ëŒì´ í¬ë©´ ê°ì‡ 
                # ëª¨ë‹¬ ë™ì˜(ë˜ëŠ” í•œ ëª¨ë‹¬ ê°•í™•ì‹ ) ì‹œì—ëŠ” ê°ì‡ í•˜ì§€ ì•ŠìŒ
                if bimodal_agree or audio_strong or text_strong:
                    continue
                if (text_support < self.ng_min_text_support) or conflict:
                    damp = self.ng_damp_factor
                    removed = cur * (1.0 - damp)
                    normalized_combined[cls] = cur * damp
                    # ì œê±°ë¶„ì˜ ëŒ€ë¶€ë¶„ì„ neutralë¡œ ì´ë™
                    normalized_combined['neutral'] = normalized_combined.get('neutral', 0.0) + removed

            # ì¬ì •ê·œí™”
            sm = sum(max(0.0, v) for v in normalized_combined.values())
            if sm > 0:
                normalized_combined = {k: max(0.0, v) / sm for k, v in normalized_combined.items()}

        best_emotion = max(normalized_combined.items(), key=lambda x: x[1])
        
        # ìƒìœ„ 2ê°œ ê°ì • ì„ íƒ (ë¡œê·¸ìš©)
        sorted_emotions = sorted(normalized_combined.items(), key=lambda x: x[1], reverse=True)[:2]

        result = EmotionResult(
            emotion=best_emotion[0],
            confidence=best_emotion[1],
            features=self._extract_audio_features(segment),
            text_score=text_scores.get(best_emotion[0], 0.0),
            audio_score=audio_scores.get(best_emotion[0], 0.0),
            text_distribution={k: float(v) for k, v in text_scores.items()},
            audio_distribution={k: float(v) for k, v in audio_scores.items()},
            combined_distribution={k: float(v) for k, v in normalized_combined.items()}
        )
        
        # ê°€ë…ì„± ì¢‹ì€ ë¡œê·¸ ì¶œë ¥
        self._log_segment_summary(segment, result, text_scores, audio_scores, combined_scores, sorted_emotions)
        
        return result

    def _extract_audio_features(self, segment: Dict[str, Any]) -> Dict[str, float]:
        """ì„¸ê·¸ë¨¼íŠ¸ë³„ ì˜¤ë””ì˜¤ íŠ¹ì„± ì¶”ì¶œ"""
        features = {}
        try:
            audio_segment = segment.get('audio', None)
            if audio_segment is not None:
                features['rms_energy'] = np.sqrt(np.mean(audio_segment**2))
                features['zero_crossing_rate'] = librosa.feature.zero_crossing_rate(audio_segment).mean()
                # ì¶”ê°€ íŠ¹ì„±ì€ í•„ìš”ì— ë”°ë¼ í™•ì¥
        except Exception as e:
            logging.warning(f"Feature extraction warning: {str(e)}")
        return features

    def _log_segment_summary(self, segment: Dict[str, Any], result: EmotionResult, 
                                   text_scores: Dict[str, float], audio_scores: Dict[str, float],
                                   combined_scores: Dict[str, float], sorted_emotions: List):
        text = segment.get('text', '').strip()
        timestamp = segment.get('start', 0)
        
        if not text:  # ë¹ˆ í…ìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°
            return
            
        # ê° ë¶„ë¥˜ë³„ ë…ë¦½ì ì¸ ìƒìœ„ 2ê°œ ê°ì • ê³„ì‚°
        # Text ê¸°ë°˜ ìƒìœ„ 2ê°œ
        sorted_text = sorted(text_scores.items(), key=lambda x: x[1], reverse=True)[:2]
        top2_text = ' | '.join([f"{emotion}: {score:.3f}" for emotion, score in sorted_text])
        
        # Audio ê¸°ë°˜ ìƒìœ„ 2ê°œ  
        sorted_audio = sorted(audio_scores.items(), key=lambda x: x[1], reverse=True)[:2]
        top2_audio = ' | '.join([f"{emotion}: {score:.3f}" for emotion, score in sorted_audio])
        
        # Combined ìƒìœ„ 2ê°œ (ê¸°ì¡´ sorted_emotions ì‚¬ìš©)
        top2_combined = ' | '.join([f"{emotion}: {score:.3f}" for emotion, score in sorted_emotions])
        
        # 3ì¤„ì„ í•œ ë¬¶ìŒìœ¼ë¡œ ì¶œë ¥ (ë°•ìŠ¤ í˜•íƒœ)
        print(f"\nâ”Œâ”€ [{timestamp:.1f}s] {text[:60]}{'...' if len(text) > 60 else ''}")
        print(f"â”œâ”€ Text:     {top2_text}")
        print(f"â”œâ”€ Audio:    {top2_audio}")
        print(f"â””â”€ Combined: {top2_combined}")
        
    def _log_segment_result(self, segment: Dict[str, Any], result: EmotionResult):
        """ê¸°ì¡´ JSON ë¡œê¹… (íŒŒì¼ìš©)"""
        log_entry = {
            'timestamp': segment.get('start', 0),
            'text': segment.get('text', ''),
            'emotion': result.emotion,
            'confidence': result.confidence,
            'text_score': result.text_score,
            'audio_score': result.audio_score
        }
        logging.info(json.dumps(log_entry, ensure_ascii=False))

    def save_results(self, results: List[EmotionResult], filepath: str):
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        output = [{
            'emotion': r.emotion,
            'confidence': r.confidence,
            'features': r.features,
            'text_score': r.text_score,
            'audio_score': r.audio_score,
            'text_distribution': r.text_distribution,
            'audio_distribution': r.audio_distribution,
            'combined_distribution': r.combined_distribution
        } for r in results]
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)

    def get_emotion_color(self, emotion: str) -> str:
        """ê°ì •ë³„ ìƒ‰ìƒ ì½”ë“œ ë°˜í™˜"""
        try:
            config_data = self.db_manager.load_config()
            emotion_colors = config_data.get('hex_colors', {}).get('emotion_colors', {})
            default_color = config_data.get('hex_colors', {}).get('default_color', '#FFFFFF')

            resolved_color = emotion_colors.get(emotion, default_color)
            
            return resolved_color 
            
        except Exception as e:
            logging.error(f"Failed to resolve emotion color from DB: {str(e)}")
            # DB ì ‘ê·¼ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            return '#FFFFFF'

    def classify_emotions(self, segments, full_audio):
        """ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œ"""
        try:
            # ì§„í–‰ ìƒí™© ì´ˆê¸° ì¶œë ¥
            print("\nê°ì • ë¶„ì„ ì¤‘...")
            print(f"ê°ì • ë¶„ë¥˜ ì§„í–‰: 0/{len(segments)}")

            # ë°°ì¹˜ ì²˜ë¦¬ ìˆ˜í–‰
            results = self.process_batch(segments, full_audio)

            # ê°ì • ë¶„ì„ ê²°ê³¼ë¥¼ ì„¸ê·¸ë¨¼íŠ¸ì— ë°˜ì˜
            for segment, result in zip(segments, results):
                segment['emotion'] = result.emotion
                segment['confidence'] = result.confidence
                segment['emotion_color'] = self.get_emotion_color(result.emotion)
                segment['features'] = result.features
                segment['text_score'] = result.text_score
                segment['audio_score'] = result.audio_score
                segment['text_scores'] = result.text_distribution
                segment['audio_scores'] = result.audio_distribution
                segment['combined_scores'] = result.combined_distribution

            # ì‹œê°„ ìŠ¤ë¬´ë”©: ê³ ë¦½ëœ happy/angry ìŠ¤íŒŒì´í¬ë¥¼ ì¤‘ë¦½ìœ¼ë¡œ ì™„í™”
            ts_cfg = config.get('emotions', 'temporal_smoothing', default={}) or {}
            if bool(ts_cfg.get('enabled', True)) and len(segments) >= 3:
                window = int(ts_cfg.get('window', 3))
                min_conf = float(ts_cfg.get('min_conf', 0.70))
                targets = set(ts_cfg.get('target_classes', ['happy', 'angry']))
                half = max(1, window // 2)
                for i in range(len(segments)):
                    seg = segments[i]
                    emo = seg.get('emotion', 'neutral')
                    if emo not in targets or float(seg.get('confidence', 0.0)) >= min_conf:
                        continue
                    # ëª¨ë‹¬ ê°•ì§€ì§€/ë™ì˜ê°€ ìˆìœ¼ë©´ ìŠ¤ë¬´ë”© ì œì™¸
                    ts = seg.get('text_scores') or {}
                    as_ = seg.get('audio_scores') or {}
                    if isinstance(ts, dict) and ts.get(emo, 0.0) >= 0.40:
                        continue
                    if isinstance(as_, dict) and as_.get(emo, 0.0) >= 0.60:
                        continue
                    # ì–‘ì˜†(ê°™ì€ í™”ì ìš°ì„ ) í™•ì¸
                    left = segments[i-1] if i-1 >= 0 else None
                    right = segments[i+1] if i+1 < len(segments) else None
                    neighbors = [s for s in [left, right] if s is not None]
                    if not neighbors:
                        continue
                    same_speaker_neighbors = [s for s in neighbors if s.get('speaker', 'Unknown') == seg.get('speaker', 'Unknown')]
                    nb = same_speaker_neighbors if same_speaker_neighbors else neighbors
                    # ì–‘ì˜† ëª¨ë‘ neutralì´ê±°ë‚˜ ìƒì´í•œ ê°ì •ì´ë©° ìì‹ ë³´ë‹¤ ì‹ ë¢°ë„ê°€ ë†’ìœ¼ë©´ ì¤‘ë¦½ìœ¼ë¡œ ì™„í™”
                    if all(n.get('emotion', 'neutral') == 'neutral' for n in nb) or all(float(n.get('confidence', 0.0)) >= float(seg.get('confidence', 0.0)) for n in nb):
                        seg['emotion'] = 'neutral'
                        seg['emotion_color'] = self.get_emotion_color('neutral')
                        seg['confidence'] = float(max(0.5, seg.get('confidence', 0.0)))
                        # ë¶„í¬ë„ë„ ì¤‘ë¦½ì— ë” ì‹±í¬
                        cs = seg.get('combined_scores') or {}
                        neu_added = 0.15
                        cs['neutral'] = float(cs.get('neutral', 0.0) + neu_added)
                        ssum = sum(max(0.0, v) for v in cs.values())
                        if ssum > 0:
                            seg['combined_scores'] = {k: float(max(0.0, v) / ssum) for k, v in cs.items()}

            # ìµœì¢… ì§„í–‰ ìƒí™© ì¶œë ¥
            print(f"ê°ì • ë¶„ë¥˜ ì§„í–‰: {len(segments)}/{len(segments)}")
            print("ê°ì • ë¶„ë¥˜ ì™„ë£Œ")

            return segments

        except Exception as e:
            logging.error(f"ê°ì • ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return segments


    def classify_audio_only(self, segments: List[Dict[str, Any]], full_audio: np.ndarray) -> List[Dict[str, Any]]:
        """
        [NEW FUNCTION]
        ì˜¤ë””ì˜¤ ëª¨ë¸ë§Œ ì‚¬ìš©í•˜ì—¬ ê°ì •ì„ ë¶„ë¥˜í•˜ê³ , í…ìŠ¤íŠ¸ ëª¨ë¸ ë° ì•™ìƒë¸” ë¡œì§ì„ ì™„ì „íˆ ë¬´ì‹œí•©ë‹ˆë‹¤.
        (ê¸°ì¡´ SubtitleGeneratorì˜ 'en' ë¡œì§ê³¼ ë™ì¼í•œ ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.)
        """
        print("\nğŸ§ ì˜¤ë””ì˜¤ ì „ìš© ê°ì • ë¶„ì„ ì‹œì‘...")

        try:
            emotion_results = self.process_batch(segments, full_audio)

            for i, segment in enumerate(segments):
                result = emotion_results[i]
            
                segment['emotion'] = result.emotion
                segment['confidence'] = result.confidence

            print(f"âœ… ì˜¤ë””ì˜¤ ì „ìš© ê°ì • ë¶„ì„ ì™„ë£Œ. ì´ {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬.")

            return segments

        except Exception as e:
            logging.error(f"Audio-Only Classification Error: {str(e)}")
            
            # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì„¸ê·¸ë¨¼íŠ¸ ë°˜í™˜
            return segments