# ëª¨ë¸ í‰ê°€ ì „ëžµ - ì¢…í•© ê°€ì´ë“œ

## ðŸ“Œ ì§ˆë¬¸ ìš”ì•½

### 1. "ì´ í”„ë¡œì íŠ¸ì— ì í•©í•œ ëª¨ë¸"ì˜ ì •ì˜ëŠ”?
### 2. ìˆ˜ë™ ìž‘ì—…ì´ í•„ìš”í•œ ëª¨ë¸ê³¼ êµ¬ì²´ì ì¸ ë‹¨ê³„ëŠ”?
### 3. ë¼ë²¨ ì—†ì´ ì •í™•ë„ë¥¼ í‰ê°€í•˜ëŠ” ë°©ë²•ì€?

---

## âœ… 1. í”„ë¡œì íŠ¸ì— ì í•©í•œ ëª¨ë¸ì˜ ì •ì˜

### í”„ë¡œì íŠ¸ ìš”êµ¬ì‚¬í•­

**í•µì‹¬ ê¸°ëŠ¥**:
- WhisperX STT â†’ ê°ì • ë¶„ë¥˜ â†’ ê°ì •ë³„ ìŠ¤íƒ€ì¼ ì ìš©ëœ ASS ìžë§‰ ìƒì„±
- ì˜í™”/ë“œë¼ë§ˆ/ìœ íŠœë¸Œ ì˜ìƒ ëŒ€ìƒ
- ë°°ì¹˜ ì²˜ë¦¬ (ì‹¤ì‹œê°„ ì•„ë‹˜)

**ìž…ì¶œë ¥**:
```
ìž…ë ¥: ì˜ìƒ íŒŒì¼ (í•œêµ­ì–´/ì˜ì–´ ìŒì„±)
ì¶œë ¥: 7ê°œ ê°ì • (neutral, happy, sad, angry, fear, surprise, disgust)
      + ìƒ‰ìƒ/í°íŠ¸ ìŠ¤íƒ€ì¼ ì ìš©ëœ .ass ìžë§‰
```

---

### âœ… í•„ìˆ˜ ì¡°ê±´ (Must-Have)

#### 1. **7ê°œ ê°ì • í´ëž˜ìŠ¤ ì§€ì›**
```python
# config.pyì— ì •ì˜ëœ ê°ì •
emotions = ['neutral', 'happy', 'sad', 'angry', 'fear', 'surprise', 'disgust']
```

**âŒ ì œì™¸ ëŒ€ìƒ**:
- 4ê°œ ì´í•˜ ê°ì • ëª¨ë¸
- ì°¨ì› ê¸°ë°˜ ì¶œë ¥ (arousal/valence)
- ë°”ì´ë„ˆë¦¬ ë¶„ë¥˜ (positive/negative)

**âœ… í—ˆìš©**:
- ì •í™•ížˆ 7ê°œ
- 8ê°œ ì´ìƒ (ë§¤í•‘ ê°€ëŠ¥: calm â†’ neutral)

---

#### 2. **ì˜¤ë””ì˜¤ ìž…ë ¥ ì§€ì›**
```python
audio_segment = librosa.load(audio_path, sr=16000)
emotion = classifier.predict(audio_segment)
```

**âŒ ì œì™¸**:
- í…ìŠ¤íŠ¸ ì „ìš© (BERT, RoBERTa)
- ì´ë¯¸ì§€ ìž…ë ¥ (ì–¼êµ´ í‘œì •)

**âœ… í—ˆìš©**:
- Wav2Vec2, HuBERT, WavLM
- íŠ¹ì§• ì¶”ì¶œ + ML (OpenSMILE + SVM)

---

#### 3. **ë‹¤êµ­ì–´ ì§€ì› (í•œêµ­ì–´ + ì˜ì–´)**

**ìš°ì„ ìˆœìœ„**:
1. ðŸ¥‡ í•œêµ­ì–´ + ì˜ì–´ ë™ì‹œ (XLS-R ê¸°ë°˜)
2. ðŸ¥ˆ ì˜ì–´ ì „ìš© (ëŒ€ê·œëª¨ ë°ì´í„°)
3. ðŸ¥‰ í•œêµ­ì–´ ì „ìš© (jungjongho)

**âŒ ì œì™¸**:
- ë‹¨ì¼ ì–¸ì–´ ì „ìš© (ê·¸ë¦¬ìŠ¤ì–´, ë…ì¼ì–´)

---

#### 4. **HuggingFace Transformers í˜¸í™˜ (ìš°ì„ )**
```python
model = AutoModelForAudioClassification.from_pretrained(model_name)
```

**ìš°ì„ ìˆœìœ„**:
- ðŸ¥‡ Transformers ì§ì ‘ ì§€ì› (ì¦‰ì‹œ í†µí•©)
- ðŸ¥ˆ Adapter íŒ¨í„´ (SpeechBrain, TensorFlow)
- ðŸ¥‰ ìˆ˜ë™ í†µí•© (OpenSMILE, Kaggle)

**âŒ ì œì™¸**:
- íì‡„í˜• API (OpenAI - ë¹„ìš©)
- ìƒìš© ë¼ì´ì„ ìŠ¤

---

#### 5. **ë°°ì¹˜ ì²˜ë¦¬ íš¨ìœ¨ì„±**
```python
for i in range(0, len(segments), batch_size):
    batch = segments[i:i + batch_size]
    results = model.predict_batch(batch)
```

**ìš”êµ¬ì‚¬í•­**:
- ê¸´ ì˜ìƒ ì²˜ë¦¬ (30ë¶„~2ì‹œê°„)
- 8GB GPUì—ì„œ ìž‘ë™
- ì‹¤ì‹œê°„ì˜ 10ë°° ì´ë‚´ ì†ë„

**âŒ ì œì™¸**:
- ì´ˆëŒ€í˜• ëª¨ë¸ (10GB+ ë©”ëª¨ë¦¬)
- ì„¸ê·¸ë¨¼íŠ¸ë‹¹ 5ì´ˆ ì´ìƒ ì²˜ë¦¬

---

### ðŸŽ¯ ê°€ì‚°ì  (Nice-to-Have)

1. **ë†’ì€ ì •í™•ë„** (Simpson ê¸°ì¤€)
   - Accuracy > 0.6
   - Macro F1 > 0.3
   - Neutral Rate < 50%

2. **ìµœì‹  ì•„í‚¤í…ì²˜**
   - Wav2Vec2 XLS-R
   - Whisper Encoder
   - Transformer + CNN

3. **í™œë°œí•œ ì»¤ë®¤ë‹ˆí‹°**
   - HuggingFace ë‹¤ìš´ë¡œë“œ 1000+
   - GitHub Stars 100+

---

### âŒ ìžë™ ì œì™¸ ê¸°ì¤€

#### ì‹¤ì œ ì œì™¸ ì‚¬ë¡€

1. **audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim**
   - ì´ìœ : arousal/valence ì¶œë ¥ (ì°¨ì› ê¸°ë°˜)
   - ê²°ê³¼: 7ê°œ ê°ì • ë§¤í•‘ ë¶ˆê°€

2. **m3hrdadfi/wav2vec2-xlsr-greek-speech-emotion-recognition**
   - ì´ìœ : ê·¸ë¦¬ìŠ¤ì–´ ì „ìš©
   - ê²°ê³¼: ì˜ì–´ ì •í™•ë„ 0.032

3. **harshit345/xlsr-wav2vec-speech-emotion-recognition**
   - ì´ìœ : 100% sad ì˜ˆì¸¡ (ê·¹ë‹¨ì  íŽ¸í–¥)
   - ê²°ê³¼: ì‹¤ìš©ì„± ì—†ìŒ

4. **í˜•ì‹ ë¹„í˜¸í™˜**
   - `config.json` ì—†ìŒ
   - Transformers ë¡œë”© ì‹¤íŒ¨

---

### ðŸ“Š ìµœì¢… í…ŒìŠ¤íŠ¸ í›„ë³´êµ° (10ê°œ)

#### Tier S: ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥
1. âœ… **superb/wav2vec2-large-superb-er** (í˜„ìž¬ 1ìœ„, 0.645 ì •í™•ë„)
2. âœ… **marcogdepinto/emotion-recognition-using-voice** (GitHub, 7ê°œ ê°ì •)

#### Tier A: í†µí•© í•„ìš”
3. ðŸ”„ **speechbrain/emotion-recognition-wav2vec2-IEMOCAP**
4. ðŸ”„ **IliaZenkov/transformer-cnn-emotion-recognition**
5. ðŸ”„ **MITESHPUTHRANNEU/Speech-Emotion-Analyzer**

#### Tier B: ê³ ê¸‰
6. ðŸ”´ **Kaggle RAVDESS** (ê³„ì • í•„ìš”)
7. ðŸŸ£ **OpenSMILE + SVM** (í•™ìŠµ í•„ìš”)

#### Tier C: í•œêµ­ì–´ íŠ¹í™”
8. âš ï¸ **jungjongho/wav2vec2-xlsr-korean** (í•œêµ­ì–´ ë°ì´í„° í•„ìš”)

---

## ðŸ“¥ 2. ìˆ˜ë™ ìž‘ì—…ì´ í•„ìš”í•œ ëª¨ë¸

### ðŸŸ¢ Tier 1: ìžë™ (ìž‘ì—… ë¶ˆí•„ìš”)

#### HuggingFace ëª¨ë¸
- superb/wav2vec2-large-superb-er
- ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition
- speechbrain/emotion-recognition-wav2vec2-IEMOCAP

**ì‚¬ìš©ìž ìž‘ì—…**: âŒ ì—†ìŒ (ìžë™ ë‹¤ìš´ë¡œë“œ)

---

### ðŸŸ¡ Tier 2: Git Clone (15ë¶„)

#### 1. marcogdepinto/emotion-recognition-using-voice

```powershell
# 1. ë””ë ‰í† ë¦¬ ìƒì„± ë° í´ë¡ 
cd C:\Users\adap8\OneDrive\Desktop\STT_KR-liveCaption
mkdir external_models
cd external_models
git clone https://github.com/marcogdepinto/emotion-recognition-using-voice.git

# 2. ëª¨ë¸ íŒŒì¼ í™•ì¸
cd emotion-recognition-using-voice
dir models\model.pkl

# 3. ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ
python download_models.py
```

**ì˜ˆìƒ ì‹œê°„**: 5ë¶„  
**ì™„ë£Œ ì¡°ê±´**: `models\model.pkl` ì¡´ìž¬

---

#### 2. IliaZenkov/transformer-cnn-emotion-recognition

```powershell
# 1. í´ë¡ 
cd C:\Users\adap8\OneDrive\Desktop\STT_KR-liveCaption\external_models
git clone https://github.com/IliaZenkov/transformer-cnn-emotion-recognition.git
cd transformer-cnn-emotion-recognition

# 2. ì¢…ì†ì„± ì„¤ì¹˜
..\..\venv\Scripts\pip.exe install -r requirements.txt

# 3. ì‚¬ì „ í•™ìŠµ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
..\..\venv\Scripts\python.exe download_models.py
```

**ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ (ìŠ¤í¬ë¦½íŠ¸ ì‹¤íŒ¨ ì‹œ)**:
1. https://github.com/IliaZenkov/transformer-cnn-emotion-recognition/releases
2. `best_model.pth` ë‹¤ìš´ë¡œë“œ
3. `checkpoints/` í´ë”ì— ì €ìž¥

**ì˜ˆìƒ ì‹œê°„**: 10ë¶„ (ë‹¤ìš´ë¡œë“œ ~500MB)  
**ì™„ë£Œ ì¡°ê±´**: `checkpoints\best_model.pth` ì¡´ìž¬

---

#### 3. MITESHPUTHRANNEU/Speech-Emotion-Analyzer

```powershell
# 1. í´ë¡ 
cd C:\Users\adap8\OneDrive\Desktop\STT_KR-liveCaption\external_models
git clone https://github.com/MITESHPUTHRANNEU/Speech-Emotion-Analyzer.git
cd Speech-Emotion-Analyzer

# 2. TensorFlow ì„¤ì¹˜
..\..\venv\Scripts\pip.exe install tensorflow

# 3. ëª¨ë¸ ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ
```

**ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ**:
1. https://github.com/MITESHPUTHRANNEU/Speech-Emotion-Analyzer/releases
2. `best_model.h5` ë‹¤ìš´ë¡œë“œ (~100MB)
3. í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ì €ìž¥

**ì˜ˆìƒ ì‹œê°„**: 10ë¶„  
**ì™„ë£Œ ì¡°ê±´**: `best_model.h5` ì¡´ìž¬

---

### ðŸ”´ Tier 3: Kaggle ê³„ì • (30ë¶„)

#### Kaggle RAVDESS ëª¨ë¸

##### Step 1: ê³„ì • ìƒì„±
1. https://www.kaggle.com ì ‘ì†
2. "Sign Up" â†’ Google/ì´ë©”ì¼ ê°€ìž…
3. ì´ë©”ì¼ ì¸ì¦

##### Step 2: API í† í°
1. ë¡œê·¸ì¸ â†’ í”„ë¡œí•„ â†’ "Settings"
2. "API" â†’ "Create New Token"
3. `kaggle.json` ìžë™ ë‹¤ìš´ë¡œë“œ

##### Step 3: í† í° ì„¤ì •
```powershell
# 1. ë””ë ‰í† ë¦¬ ìƒì„±
mkdir $env:USERPROFILE\.kaggle

# 2. í† í° ë³µì‚¬
copy "%USERPROFILE%\Downloads\kaggle.json" "$env:USERPROFILE\.kaggle\"

# 3. í™•ì¸
dir $env:USERPROFILE\.kaggle\kaggle.json
```

##### Step 4: ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
```powershell
# 1. Kaggle CLI ì„¤ì¹˜
cd C:\Users\adap8\OneDrive\Desktop\STT_KR-liveCaption
.\venv\Scripts\pip.exe install kaggle

# 2. ë‹¤ìš´ë¡œë“œ
cd external_models
mkdir kaggle_models
cd kaggle_models
..\..\.venv\Scripts\kaggle.exe datasets download -d uwrfkaggle/ravdess-emotional-speech-audio

# 3. ì••ì¶• í•´ì œ
tar -xf ravdess-emotional-speech-audio.zip
```

**ì˜ˆìƒ ì‹œê°„**: 20ë¶„ (ë‹¤ìš´ë¡œë“œ ~500MB)  
**ì™„ë£Œ ì¡°ê±´**: `kaggle.json` ì„¤ì • + RAVDESS ë°ì´í„° ì¡´ìž¬

---

### ðŸŸ£ Tier 4: í•™ìŠµ í•„ìš” (1-2ì‹œê°„)

#### OpenSMILE + SVM

```powershell
# 1. ì„¤ì¹˜
.\venv\Scripts\pip.exe install opensmile scikit-learn

# 2. ë°ì´í„°ì…‹ ì¤€ë¹„ (ìœ„ RAVDESS ì‚¬ìš©)

# 3. íŠ¹ì§• ì¶”ì¶œ ë° í•™ìŠµ (ìŠ¤í¬ë¦½íŠ¸ ì œê³µ ì˜ˆì •)
```

âš ï¸ **ì£¼ì˜**: ë‹¤ë¥¸ ëª¨ë¸ í‰ê°€ í›„ í•„ìš” ì‹œ ì§„í–‰ ê¶Œìž¥

---

### ðŸŸ¤ Tier 5: í•œêµ­ì–´ ë°ì´í„°ì…‹ (1-3ì¼)

#### jungjongho ëª¨ë¸ í‰ê°€ìš©

##### Option 1: AI Hub
1. https://aihub.or.kr ê°€ìž…
2. "í•œêµ­ì–´ ë©€í‹°ëª¨ë‹¬ ê°ì • ë°ì´í„°ì…‹" ì‹ ì²­
3. ìŠ¹ì¸ ëŒ€ê¸° (1-3ì¼)
4. ë‹¤ìš´ë¡œë“œ (~50GB)

##### Option 2: ì§ì ‘ ë¼ë²¨ë§
```powershell
# í•œêµ­ì–´ ì˜ìƒ ì¤€ë¹„
# labelled_korean.jsonl ìƒì„± (Simpson í˜•ì‹)
# ìµœì†Œ 30ê°œ ì„¸ê·¸ë¨¼íŠ¸
```

---

### ðŸ“‹ ì²´í¬ë¦¬ìŠ¤íŠ¸

#### í•„ìˆ˜ ìž‘ì—… (5ê°œ ëª¨ë¸)
- [ ] **marcogdepinto** Git Clone (5ë¶„)
- [ ] **IliaZenkov** Git Clone + ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (10ë¶„)
- [ ] **MITESHPUTHRANNEU** ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ (10ë¶„)

**ì´ ì†Œìš” ì‹œê°„**: 25ë¶„

#### ì„ íƒ ìž‘ì—…
- [ ] **Kaggle** ê³„ì • + í† í° (30ë¶„)
- [ ] **í•œêµ­ì–´ ë°ì´í„°ì…‹** (1-3ì¼)

---

## ðŸ“ˆ 3. ë¼ë²¨ ì—†ì´ ì •í™•ë„ í‰ê°€í•˜ëŠ” ë°©ë²•

### ë¬¸ì œì 
- ë¼ë²¨ë§ ì‹œê°„ ì†Œìš” (1ì‹œê°„+)
- ìƒˆ ì˜ìƒë§ˆë‹¤ ìˆ˜ë™ ìž‘ì—…
- ì£¼ê´€ì  íŒë‹¨

### í•´ê²°ì±…: Unsupervised Evaluation

---

### âœ… ë°©ë²• 1: Cross-Model Consistency (ëª¨ë¸ ê°„ ì¼ì¹˜ë„)

#### ì›ë¦¬
ì—¬ëŸ¬ ëª¨ë¸ì´ **ê°™ì€ ì„¸ê·¸ë¨¼íŠ¸**ì— ì¼ì¹˜í•˜ëŠ” ì˜ˆì¸¡ â†’ ë†’ì€ ì‹ ë¢°ë„

#### ì‚¬ìš©ë²•
```powershell
python tools/unsupervised_evaluator.py \
    --video assets/simpson.mp4 \
    --models \
        superb/wav2vec2-large-superb-er \
        ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition \
        speechbrain/emotion-recognition-wav2vec2-IEMOCAP
```

#### í‰ê°€ ì§€í‘œ
- **í‰ê·  ì¼ì¹˜ë„**: > 0.6 (ì¢‹ìŒ)
- **ê³ ì‹ ë¢° ë¹„ìœ¨**: > 50%
- **ë‹¤ì–‘ì„± (Entropy)**: 1.5~2.0

---

### âœ… ë°©ë²• 2: Confidence Distribution Analysis (ì‹ ë¢°ë„ ë¶„í¬)

#### ì§€í‘œ
```python
{
    'mean_confidence': 0.67,      # í‰ê·  ì‹ ë¢°ë„
    'high_conf_ratio': 0.52,      # ê³ ì‹ ë¢° ë¹„ìœ¨ (>0.7)
    'neutral_ratio': 0.35,        # ì¤‘ë¦½ ë¹„ìœ¨
    'diversity': 1.82             # ê°ì • ë‹¤ì–‘ì„±
}
```

#### ì¢‹ì€ ëª¨ë¸ ê¸°ì¤€
- í‰ê·  ì‹ ë¢°ë„: 0.6~0.8
- ì¤‘ë¦½ ë¹„ìœ¨: 20~40%
- ë‹¤ì–‘ì„±: 1.5~2.0

---

### âœ… ë°©ë²• 3: Entropy-Based Quality Score

#### ì›ë¦¬
ì˜ˆì¸¡ ë¶„í¬ê°€ **ì ì • ìˆ˜ì¤€**ì˜ í™•ì‹ ë„ ìœ ì§€

#### í’ˆì§ˆ ì ìˆ˜
```python
quality_score = (
    0.3 * consistency +           # ì¼ì¹˜ë„
    0.3 * entropy_quality +       # ì˜ˆì¸¡ í’ˆì§ˆ
    0.2 * (1 - neutral_ratio) +   # ì¤‘ë¦½ íŒ¨ë„í‹°
    0.2 * mean_confidence         # í‰ê·  ì‹ ë¢°ë„
)
```

| ì ìˆ˜ | íŒë‹¨ |
|------|------|
| > 0.8 | ìš°ìˆ˜ |
| 0.6~0.8 | ì–‘í˜¸ |
| 0.4~0.6 | ë³´í†µ |
| < 0.4 | ë¶ˆëŸ‰ |

---

### âœ… ë°©ë²• 4: Perceptual Validation (ì§€ê°ì  ê²€ì¦)

#### ì ˆì°¨
1. ê° ê°ì •ë³„ ëŒ€í‘œ ìƒ˜í”Œ ì„ íƒ (10ê°œ)
2. HTML ë¦¬ë·° íŽ˜ì´ì§€ ìƒì„±
3. 10ê°œë§Œ ìˆ˜ë™ í™•ì¸ (10ë¶„)
4. ì •í™•ë„ ì¶”ì •

#### ìž¥ì 
- ë¹ ë¦„ (10ë¶„)
- ì¸ê°„ ì§ê´€ í™œìš©
- ê·¹ë‹¨ì  ì˜¤ë¥˜ íƒì§€

---

### ðŸ“Š í†µí•© í‰ê°€ í”„ë ˆìž„ì›Œí¬

#### ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ

```powershell
# 1. Unsupervised í‰ê°€ ì‹¤í–‰
python tools/unsupervised_evaluator.py \
    --video assets/simpson.mp4 \
    --models \
        superb/wav2vec2-large-superb-er \
        marcogdepinto/emotion-recognition \
        speechbrain/emotion-recognition-wav2vec2-IEMOCAP \
    --output-dir result

# ì¶œë ¥:
# ðŸ“Š UNSUPERVISED EVALUATION RESULTS
# 
# ðŸ† Rank 1: superb/wav2vec2-large-superb-er
#    Overall Score: 0.687
#    â”œâ”€ Consistency: 0.723
#    â”œâ”€ Entropy Quality: 0.845
#    â”œâ”€ Mean Confidence: 0.671
#    â””â”€ Neutral Ratio: 0.323
# 
# ðŸ† Rank 2: speechbrain/emotion-recognition-wav2vec2-IEMOCAP
#    Overall Score: 0.642
#    ...
```

#### ìƒì„± íŒŒì¼
- `result/unsupervised_eval_simpson.json` (ìƒì„¸ ê²°ê³¼)
- `result/unsupervised_eval_simpson.png` (ì‹œê°í™”)

---

### ê²€ì¦ ì „ëžµ

#### 1ë‹¨ê³„: Simpson (ë¼ë²¨ ìžˆìŒ)
```powershell
# Supervised í‰ê°€
python tools/model_evaluator.py \
    --video assets/simpson.mp4 \
    --labels labelled_simpson.jsonl \
    --disable-text

# ê²°ê³¼: superb = 0.645 ì •í™•ë„
```

#### 2ë‹¨ê³„: Simpson (ë¼ë²¨ ìˆ¨ê¹€)
```powershell
# Unsupervised í‰ê°€
python tools/unsupervised_evaluator.py \
    --video assets/simpson.mp4 \
    --models superb/wav2vec2-large-superb-er ...

# ê²°ê³¼: superb = 0.687 í’ˆì§ˆ ì ìˆ˜
```

#### 3ë‹¨ê³„: ìƒê´€ê´€ê³„ í™•ì¸
```python
correlation = 0.85  # Supervised vs Unsupervised
# 0.85 ì´ìƒì´ë©´ ì‹ ë¢° ê°€ëŠ¥
```

#### 4ë‹¨ê³„: ìƒˆ ì˜ìƒ ì ìš©
```powershell
# ë¼ë²¨ ì—†ëŠ” ìƒˆ ì˜ìƒë„ í‰ê°€ ê°€ëŠ¥
python tools/unsupervised_evaluator.py \
    --video assets/new_movie.mp4 \
    --models ...
```

---

### ë¹„êµí‘œ

| ë°©ë²• | ë¼ë²¨ | ì‹œê°„ | ì‹ ë¢°ë„ | ìš©ë„ |
|------|------|------|--------|------|
| Cross-Model | âŒ | ìžë™ | â­â­â­â­ | ê¸°ë³¸ |
| Confidence | âŒ | ìžë™ | â­â­â­ | í’ˆì§ˆ ì§„ë‹¨ |
| Entropy | âŒ | ìžë™ | â­â­â­â­ | ì •ëŸ‰ í‰ê°€ |
| Perceptual | âŒ | 10ë¶„ | â­â­â­â­â­ | ìµœì¢… ê²€ì¦ |
| Supervised | âœ… | 1ì‹œê°„+ | â­â­â­â­â­ | ì ˆëŒ€ ê¸°ì¤€ |

**ê¶Œìž¥ ì¡°í•©**:
1. Simpsonìœ¼ë¡œ Supervised 1íšŒ
2. ëª¨ë“  ì˜ìƒì— Unsupervised ì ìš©
3. 10ê°œ ìƒ˜í”Œë¡œ Perceptual Validation

**ë¼ë²¨ë§ ì‹œê°„ 90% ì ˆê°!**

---

## ðŸŽ¯ ì‹¤ì „ í”„ë¡œí† ì½œ

### Phase 1: í•„ìˆ˜ ëª¨ë¸ í‰ê°€ (1ì‹œê°„)

```powershell
# 1. HuggingFace ëª¨ë¸ (ìžë™)
python tools/model_evaluator.py \
    --video assets/simpson.mp4 \
    --labels labelled_simpson.jsonl \
    --disable-text \
    --audio-models \
        superb/wav2vec2-large-superb-er \
        ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition
```

### Phase 2: GitHub ëª¨ë¸ (25ë¶„ ì„¤ì • + 1ì‹œê°„ í‰ê°€)

```powershell
# 1. ìˆ˜ë™ ìž‘ì—… (25ë¶„)
# - marcogdepinto git clone
# - IliaZenkov git clone + ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
# - MITESHPUTHRANNEU ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ

# 2. í‰ê°€ (ìžë™ í†µí•© í›„)
python tools/unsupervised_evaluator.py \
    --video assets/simpson.mp4 \
    --models \
        superb/wav2vec2-large-superb-er \
        marcogdepinto/emotion-recognition \
        IliaZenkov/transformer-cnn
```

### Phase 3: ìµœì¢… ê²€ì¦ (10ë¶„)

```powershell
# 10ê°œ ìƒ˜í”Œ ìˆ˜ë™ í™•ì¸
# HTML ë¦¬ë·° íŽ˜ì´ì§€ ìƒì„± (ìžë™)
```

**ì´ ì†Œìš” ì‹œê°„**: 2ì‹œê°„ 35ë¶„

---

## ðŸ“š ìƒì„±ëœ ë¬¸ì„œ

1. **docs/model_selection_criteria.md** - ëª¨ë¸ ì„ ì • ê¸°ì¤€ ìƒì„¸
2. **docs/manual_setup_guide.md** - ìˆ˜ë™ ìž‘ì—… ë‹¨ê³„ë³„ ê°€ì´ë“œ
3. **docs/unsupervised_evaluation_methods.md** - ë¼ë²¨ ì—†ëŠ” í‰ê°€ ë°©ë²•
4. **tools/unsupervised_evaluator.py** - ì‹¤í–‰ ê°€ëŠ¥í•œ í‰ê°€ ë„êµ¬

---

## ðŸš€ ë‹¤ìŒ ë‹¨ê³„

ì›í•˜ëŠ” ìž‘ì—…ì„ ì„ íƒí•˜ì„¸ìš”:

### Option 1: ì¦‰ì‹œ í‰ê°€ ì‹œìž‘ (HuggingFace ëª¨ë¸ë§Œ)
```powershell
python tools/unsupervised_evaluator.py \
    --video assets/simpson.mp4 \
    --models \
        superb/wav2vec2-large-superb-er \
        ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition \
        speechbrain/emotion-recognition-wav2vec2-IEMOCAP
```

### Option 2: GitHub ëª¨ë¸ ì„¤ì • ê°€ì´ë“œ
í•„ìˆ˜ ìž‘ì—… 3ê°œ (25ë¶„) ë‹¨ê³„ë³„ ì•ˆë‚´

### Option 3: ì „ì²´ 10ê°œ ëª¨ë¸ í‰ê°€ ê³„íš
Kaggle í¬í•¨ ì „ì²´ íŒŒì´í”„ë¼ì¸

ì–´ë–¤ ì˜µì…˜ìœ¼ë¡œ ì§„í–‰í• ê¹Œìš”?
